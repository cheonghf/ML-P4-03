{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXX2fhWkveIr4AFZPRpek+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cheonghf/ML-P4-03/blob/main/V2_Project_SourceCode_P4_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr87tkF0df2f"
      },
      "source": [
        "Import Salary Data CSV from Kaggle to Colab as Dataframe\n",
        "\n",
        "*   Dataset is sourced from https://www.kaggle.com/datasets/mohithsairamreddy/salary-data/data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffo7mgqpf0NV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Download latest version\n",
        "dataset_ref = kagglehub.dataset_download('mohithsairamreddy/salary-data', path='Salary_Data.csv')\n",
        "\n",
        "#dataframe salary data 1\n",
        "df_sd1 = pd.read_csv(dataset_ref)\n",
        "\n",
        "#copy dataframe salary data 1 to dataframe salary data 2\n",
        "df_sd2 = df_sd1.copy()\n",
        "\n",
        "print('Original Data Frame:', df_sd1.shape)\n",
        "print('Copied Data Frame:', df_sd2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWtPtZh5O0g4"
      },
      "source": [
        "#Step 1: Pre-Processing of Data\n",
        "\n",
        "*   Run Everything\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-bUx4Co2O4z"
      },
      "source": [
        "View Columns of 'Salary Data' Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBVDEYAg2T5G"
      },
      "outputs": [],
      "source": [
        "print(df_sd2.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uorwj6Qt2aR2"
      },
      "source": [
        "Rename Columns in 'Salary Data' Dataframe by simplyfing and removing spaces, this allows us to better track for ease of programming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRnAan2l2dex"
      },
      "outputs": [],
      "source": [
        "df_sd2.rename(columns={\"Education Level\": \"EduLevel\", \"Job Title\": \"JobTitle\", \"Years of Experience\": \"YrsExp\"}, inplace=True)\n",
        "print(\"Updated Columns are:\", df_sd2.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZI_76zcEUYx"
      },
      "source": [
        "## Step 1.1: What did we do if our Dataframe has Null values?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIa-uJFgdnsv"
      },
      "source": [
        "View the full 'Salary Data' Dataframe and Identify if it holds any Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plbhaO5agG5o"
      },
      "outputs": [],
      "source": [
        "# view dataset salary data 2\n",
        "print(df_sd2)\n",
        "print(\"===Data Frame End===\")\n",
        "print('')\n",
        "\n",
        "# Identify columns with missing valuesin dataset salary data 2\n",
        "print('Null Values are at:')\n",
        "print(df_sd2.isnull().sum())\n",
        "print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No2GkgGBeg83"
      },
      "source": [
        "1. Remove identified Null Values from the Dataframe\n",
        "2. Run a check on the DataFrame to confirm if the Null Values are removed\n",
        "3. We will go a step further and check if hidden Null Values could be represented by a string 'Na'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3enNgXyen-2"
      },
      "outputs": [],
      "source": [
        "# Remove rows with missing values\n",
        "df_sd2.dropna(inplace=True)\n",
        "\n",
        "# Check if columns with missing values are removed\n",
        "print(df_sd2.isnull().sum())\n",
        "print('')\n",
        "\n",
        "# To check if 'na' is string instead of just null\n",
        "for column in df_sd2.columns:\n",
        "    na_count = df_sd2[column].astype(str).apply(lambda x: x == 'na').sum()\n",
        "    if na_count > 0:\n",
        "        print(f\"Column '{column}' contains {na_count} rows with the string 'na'\")\n",
        "    else:\n",
        "      print(f\"Column '{column}' does not contain the string 'na'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LKEVWO1Eo1j"
      },
      "source": [
        "## Step 1.2: What did we do if our Dataframe has Duplicate Entries?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VNAsH8XYNHh"
      },
      "source": [
        "Check for Duplicate Entries in 'Salary Data' Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7EBl8oqXfz9"
      },
      "outputs": [],
      "source": [
        "# Check for duplicates and create a boolean Series\n",
        "duplicates = df_sd2.duplicated()\n",
        "\n",
        "# Count the number of duplicate rows\n",
        "num_duplicates = duplicates.sum()\n",
        "\n",
        "# Calculate the number of non-duplicate rows\n",
        "num_non_duplicates = len(df_sd2) - num_duplicates\n",
        "\n",
        "# Calculate the percentage of duplicates\n",
        "percentage_duplicates = (num_duplicates / len(df_sd2)) * 100\n",
        "\n",
        "# Print the results\n",
        "print(f\"Number of duplicate rows: {num_duplicates}\")\n",
        "print(f\"Number of non-duplicate rows: {num_non_duplicates}\")\n",
        "print(f\"Percentage of duplicate rows: {percentage_duplicates:.2f}%\")\n",
        "print('')\n",
        "\n",
        "# View the actual duplicate rows:\n",
        "if num_duplicates > 0:\n",
        "  print(\"Duplicate rows:\")\n",
        "  print(df_sd2[duplicates])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM4_K6_Cb0XJ"
      },
      "source": [
        "Filter Duplicate Rows of Data into another Data Frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlxPmQfVbz9w"
      },
      "outputs": [],
      "source": [
        "# Checking duplicates distribution\n",
        "duplicate_counts = df_sd2[df_sd2.duplicated()].groupby(df_sd2.columns.tolist()).size().reset_index(name=\"Count\")\n",
        "\n",
        "# Identifying potential duplication patterns\n",
        "df_duplicates = df_sd2.groupby(df_sd2.columns.tolist()).size().reset_index(name='Duplicate Count')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxgbJ0w-cZNH"
      },
      "source": [
        "Histogram Comparison for 'Age' Duplicate Filter Before Vs After"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GF0xEox3cJIT"
      },
      "outputs": [],
      "source": [
        "# Visualising Age distribution before and after removing duplicates\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Before removing duplicates\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_sd2['Age'].dropna(), bins=20, kde=True, color='blue')\n",
        "plt.title(\"Age Distribution Before Removing Duplicates\")\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "# After removing duplicates\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(df_duplicates['Age'].dropna(), bins=20, kde=True, color='green')\n",
        "plt.title(\"Age Distribution After Removing Duplicates\")\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZQc154obbd0"
      },
      "source": [
        "Histogram Comparison for 'Salary' Duplicate Filter Before Vs After"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYbfXkERZ51h"
      },
      "outputs": [],
      "source": [
        "# Visualising salary distribution before and after removing duplicates\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Before removing duplicates\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_sd2['Salary'].dropna(), bins=20, kde=True, color='blue')\n",
        "plt.title(\"Salary Distribution Before Removing Duplicates\")\n",
        "plt.xlabel(\"Salary\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "# After removing duplicates\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(df_duplicates['Salary'].dropna(), bins=20, kde=True, color='green')\n",
        "plt.title(\"Salary Distribution After Removing Duplicates\")\n",
        "plt.xlabel(\"Salary\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elrt9zFKkej8"
      },
      "source": [
        "Histogram Comparison for 'YrsExp' Duplicate Filter Before Vs After"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsl-qqb1cj7-"
      },
      "outputs": [],
      "source": [
        "# Visualising Years of Experience distribution before and after removing duplicates\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Before removing duplicates\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_sd2['YrsExp'].dropna(), bins=20, kde=True, color='blue')\n",
        "plt.title(\"Years of Experience Distribution Before Removing Duplicates\")\n",
        "plt.xlabel(\"Years of Experience\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "# After removing duplicates\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(df_duplicates['YrsExp'].dropna(), bins=20, kde=True, color='green')\n",
        "plt.title(\"Years of Experience Distribution After Removing Duplicates\")\n",
        "plt.xlabel(\"Years of Experience\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba57JNxgdsVV"
      },
      "source": [
        "Removing Duplicates from the Dataframe\n",
        "\n",
        "*   We chose to remove them to prevent our model from overfitting\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPqqrS87dvpX"
      },
      "outputs": [],
      "source": [
        "# Remove duplicate rows and keep the first occurrence\n",
        "df_sd3 = df_sd2.drop_duplicates(keep='first')\n",
        "\n",
        "# Reset the index\n",
        "df_sd3 = df_sd3.reset_index(drop=True)\n",
        "\n",
        "# Print the shape of the DataFrame before and after removing duplicates\n",
        "print(\"Original DataFrame shape:\", df_sd2.shape)\n",
        "print(\"DataFrame shape after removing duplicates:\", df_sd3.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj6pWLjbFCdI"
      },
      "source": [
        "## Step 1.3: Identifying if there are any neglible data to remove from the 'Salary Data' Dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSpm8HnskvEj"
      },
      "source": [
        "Understanding values inside 'Salary Data' Dataframe\n",
        "\n",
        "* Data Type in each columns\n",
        "* Summary Statistic for each numerical columns\n",
        "* Frequency count for unique data in each columns\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KJj2Mh1ex7Q"
      },
      "outputs": [],
      "source": [
        "# view dataset salary data 3 information\n",
        "print(df_sd3.info())\n",
        "print('')\n",
        "\n",
        "# Statistic range of dataset salary data 3\n",
        "print(df_sd3.describe())\n",
        "print('')\n",
        "\n",
        "string_columns = df_sd3.select_dtypes(include=['object']).columns\n",
        "\n",
        "for column in string_columns:\n",
        "    frequency_table = df_sd3[column].value_counts().to_frame()\n",
        "    print(f\"Column: {column}\")\n",
        "    print(frequency_table)  # Optionally add .to_string() for better formatting\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VccH4825TlcZ"
      },
      "source": [
        "To determine if 'Other' in 'Gender' column is significant ( less than 2%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-trmUDVwP8jy"
      },
      "outputs": [],
      "source": [
        "gender_counts = df_sd3['Gender'].value_counts()\n",
        "other_percentage = (gender_counts['Other'] / len(df_sd3)) * 100\n",
        "print(f\"Percentage of 'Other' in Gender: {other_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PakStBNOKnv"
      },
      "source": [
        "Remove 'Other' and its relevant row of data from the column 'Gender'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcBnmeV9OM-h"
      },
      "outputs": [],
      "source": [
        "# Filter out rows where Gender is 'Other'\n",
        "df_sd3 = df_sd3[df_sd3['Gender'] != 'Other']\n",
        "\n",
        "# Reset the index if needed\n",
        "df_sd3 = df_sd3.reset_index(drop=True)\n",
        "\n",
        "# Display unique values after cleaning\n",
        "print('Updated Unique Values under \"Gender\":', df_sd3['Gender'].unique())\n",
        "\n",
        "# Display shape\n",
        "print(\"DataFrame shape after cleaning Gender:\", df_sd3.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi-GCmbZRfwq"
      },
      "source": [
        "Standardise naming of Data under the column 'EduLevel'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtrzRi6vRe8k"
      },
      "outputs": [],
      "source": [
        "# Define a mapping dictionary for standardisation\n",
        "education_mapping = {\n",
        "    \"Bachelor's Degree\": \"BachelorDegree\",\n",
        "    \"Bachelor's\": \"BachelorDegree\",\n",
        "    \"Master's Degree\": \"MasterDegree\",\n",
        "    \"Master's\": \"MasterDegree\",\n",
        "    \"phD\": \"PhD\",\n",
        "    \"High School\": \"HighSchool\"\n",
        "}\n",
        "\n",
        "# Apply mapping to clean the 'EduLevel' column\n",
        "df_sd3['EduLevel'] = df_sd3['EduLevel'].replace(education_mapping)\n",
        "\n",
        "# Display unique values after cleaning\n",
        "print('Updated Unique Values in Data Frame:', df_sd3['EduLevel'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MUIgMK0OiC8"
      },
      "source": [
        "# Step 2: Analsying the DataSet (Run Step 1 Before this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I57xItm91LrF"
      },
      "source": [
        "Grouped Bar Chart: Gender Breakdown by Education Level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQc-4sr4ptJN"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))  # Adjust figure size if needed\n",
        "sns.countplot(x='EduLevel', hue='Gender', data=df_sd3)\n",
        "plt.title('Breakdown of Gender by Education Level')\n",
        "plt.xlabel('Education Level')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK-AqpBnPdNy"
      },
      "source": [
        "Histogram: Age Distribution of Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QSiLeROOKCj"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))  # Adjust figure size if needed\n",
        "sns.histplot(df_sd3['Age'], bins=20, kde=True)  # 'bins' controls the number of bars, 'kde' adds a density curve\n",
        "plt.title('Age Distribution')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tSsrUJS9rC7"
      },
      "source": [
        "BoxPlot: Salary Distribution by Gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUJ4MtiL9j-9"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))  # Adjust figure size if needed\n",
        "sns.boxplot(x='Gender', y='Salary', data=df_sd3)\n",
        "plt.title('Salary Distribution by Gender')\n",
        "plt.xlabel('Gender')\n",
        "plt.ylabel('Salary')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXIvlyQ11XFs"
      },
      "source": [
        "Box Plot Salary Distribution by Education Level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FZghjFc1TCJ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='EduLevel', y='Salary', data=df_sd3)\n",
        "plt.title('Salary Distribution by Education Level')\n",
        "plt.xlabel('Education Level')\n",
        "plt.ylabel('Salary')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsN7n9nD2Aw9"
      },
      "source": [
        "Scatterplot of Salary Distribution by Years of Experience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQt2f2Ei11gp"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.scatterplot(x='YrsExp', y='Salary', data=df_sd3)\n",
        "plt.title('Salary Distribution by Years of Experience')\n",
        "plt.xlabel('Years of Experience')\n",
        "plt.ylabel('Salary')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f1LF4UElM7m"
      },
      "source": [
        "Scatterplot of Salary Distribution by Age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB5gF22Z2kOd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.scatterplot(x='Age', y='Salary', data=df_sd3)\n",
        "plt.title('Salary Distribution by Age')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Salary')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_wtpsIwB6qW"
      },
      "source": [
        "Heatmap for numerical feature relationships of Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL37CbvIBX-Q"
      },
      "outputs": [],
      "source": [
        "# Show correlation heat map and matrix\n",
        "df_corr = df_sd3[['Age','YrsExp','Salary']]\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "corrmat = df_corr.corr()\n",
        "\n",
        "# Title\n",
        "plt.title(\"Numerical Correlation Heatmap\")\n",
        "\n",
        "# Generate the heatmap\n",
        "sns.heatmap(corrmat, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXxIg47tBZHY"
      },
      "source": [
        "Heatmap for categorical features relationships of Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bS_0kqQ_0ob"
      },
      "outputs": [],
      "source": [
        "def cramers_v(confusion_matrix):\n",
        "    \"\"\"Calculate Cramér's V statistic for association between two categorical variables\"\"\"\n",
        "    chi2 = stats.chi2_contingency(confusion_matrix)[0]  # Get Chi-square value\n",
        "    n = confusion_matrix.sum().sum()  # Total observations\n",
        "    k = min(confusion_matrix.shape)  # Minimum of row/column count\n",
        "    return np.sqrt(chi2 / (n * (k - 1)))\n",
        "\n",
        "# List of categorical variables\n",
        "categorical_vars = ['Gender', 'EduLevel', 'JobTitle']\n",
        "\n",
        "# Compute Cramér's V matrix\n",
        "cramers_v_matrix = pd.DataFrame(np.zeros((len(categorical_vars), len(categorical_vars))),\n",
        "                                index=categorical_vars, columns=categorical_vars)\n",
        "\n",
        "for i, var1 in enumerate(categorical_vars):\n",
        "    for j, var2 in enumerate(categorical_vars):\n",
        "        if i == j:\n",
        "            cramers_v_matrix.iloc[i, j] = 1.0  # Diagonal should be 1 (same variable)\n",
        "        else:\n",
        "            contingency_table = pd.crosstab(df_sd3[var1], df_sd3[var2])\n",
        "            cramers_v_matrix.iloc[i, j] = cramers_v(contingency_table)\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cramers_v_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Cramér's V Heatmap for Categorical Variables\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pldeuQ8wU38p"
      },
      "source": [
        "# Step 3: Machine Learning Algorithm Implementation with Raw Data (Run Step 1 Before this)\n",
        "\n",
        "1. We will encode the relevant columns of data that fit our problem statement\n",
        "2. Next, we will spilt the Dataset\n",
        "3. Lastly, we will run 5 different types of Machine Learning Algorithms to identify best two\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l6GPRn2803i"
      },
      "source": [
        "Copy Dataframe for Step 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOfitDdJ8udm"
      },
      "outputs": [],
      "source": [
        "df_sd4 = df_sd3.copy()\n",
        "\n",
        "print('Original Data Frame:', df_sd3.shape)\n",
        "print('Copied Data Frame:', df_sd4.shape)\n",
        "print()\n",
        "df_sd4.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjvnD9tkWUwI"
      },
      "source": [
        "Encoding raw data from pre-processed 'Salary Data' Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EN2n-hnwWhgY"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
        "\n",
        "# Drop 'JobTitle' (high cardinality, too specific)\n",
        "df_sd5 = df_sd4.drop(columns=['JobTitle'])\n",
        "\n",
        "# Separate features and target\n",
        "X = df_sd5.drop(columns=['Salary'])\n",
        "y = df_sd5['Salary']\n",
        "\n",
        "# Identify feature types\n",
        "categorical_ordinal = ['EduLevel']  # Ordinal categories\n",
        "categorical_nominal = ['Gender']  # Nominal categories\n",
        "numerical_cols = ['Age', 'YrsExp']\n",
        "\n",
        "# Apply Label Encoding to ordinal categorical variables\n",
        "for col in categorical_ordinal:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Apply OneHotEncoding to nominal categorical variables\n",
        "ohe = OneHotEncoder(sparse_output=False)  # Avoid dummy variable trap\n",
        "X_encoded = ohe.fit_transform(X[categorical_nominal])\n",
        "encoded_feature_names = ohe.get_feature_names_out(categorical_nominal)\n",
        "\n",
        "# Scale numerical variables\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X[numerical_cols])\n",
        "\n",
        "# Combine all transformed features\n",
        "X_processed = np.hstack((X_scaled, X[categorical_ordinal].values, X_encoded))\n",
        "\n",
        "# Convert to DataFrame for clarity\n",
        "X_final = pd.DataFrame(X_processed, columns=numerical_cols + categorical_ordinal + list(encoded_feature_names))\n",
        "\n",
        "X_final.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVfnmQQc4cd_"
      },
      "source": [
        "Spilting of Training Data (Must Run before Algorithm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1947CumL4bB1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Train-test split (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "043PF1TbG6je"
      },
      "source": [
        "## Step 3.1: Among the five, we will pick the best two performing Machine Learning Algorithms to improve on it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNAr7a204xEn"
      },
      "source": [
        "**Algorithm 1**: LinearRegression (Train), **Algorithm 2**: DecisionTreeRegressor (Train), **Algorithm 3**: RandomForestRegressor (Train), **Algorithm 4**: XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DLcTzLVxRFD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import psutil\n",
        "import tracemalloc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def measure_training_time(model, X_train, y_train):\n",
        "    cpu_before = psutil.cpu_percent(interval=None)\n",
        "    tracemalloc.start()\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    end_time = time.time()\n",
        "    cpu_after = psutil.cpu_percent(interval=None)\n",
        "    current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    peak_ram = peak_memory / (1024 * 1024)  # Convert to MB\n",
        "    training_time = end_time - start_time\n",
        "    avg_cpu = (cpu_before + cpu_after) / 2\n",
        "\n",
        "    print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "    print(f\"Average CPU Usage: {avg_cpu:.2f}%\")\n",
        "    print(f\"Peak RAM Usage: {peak_ram:.2f} MB\")\n",
        "\n",
        "    return training_time, avg_cpu, peak_ram\n",
        "\n",
        "def measure_inference_time(model, X_test):\n",
        "    start_time = time.time()\n",
        "    y_pred = model.predict(X_test)\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    avg_time = total_time / len(X_test)\n",
        "    print(f\"Total Inference Time: {total_time:.2f} seconds\")\n",
        "    print(f\"Average Inference Time per Sample: {avg_time:.6f} seconds\")\n",
        "    return total_time, avg_time, y_pred\n",
        "\n",
        "def measure_memory_usage():\n",
        "    process = psutil.Process()\n",
        "    memory_usage = process.memory_info().rss / (1024 * 1024)\n",
        "    print(f\"Memory Usage: {memory_usage:.2f} MB\")\n",
        "    return memory_usage\n",
        "\n",
        "def evaluate_model(y_test, y_pred, model_name):\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(f\"{model_name} Performance:\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "    return mae, mse, r2\n",
        "\n",
        "def run_models(X_train, y_train, X_test, y_test):\n",
        "    models = {\n",
        "        \"Linear Regression\": LinearRegression(),\n",
        "        \"Decision Tree\": DecisionTreeRegressor(max_depth=5, random_state=42),\n",
        "        \"Random Forest\": RandomForestRegressor(random_state=42),\n",
        "        \"XGBoost\": XGBRegressor(n_estimators=200, learning_rate=0.05, random_state=42),\n",
        "        \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, random_state=42)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"Running {model_name}...\")\n",
        "        train_time, avg_cpu, peak_ram = measure_training_time(model, X_train, y_train)\n",
        "        inf_time, avg_inf_time, y_pred = measure_inference_time(model, X_test)\n",
        "        mae, mse, r2 = evaluate_model(y_test, y_pred, model_name)\n",
        "        memory_usage = measure_memory_usage()\n",
        "\n",
        "        results[model_name] = {\n",
        "            \"Training Time (s)\": train_time,\n",
        "            \"Average CPU Usage (%)\": avg_cpu,\n",
        "            \"Peak RAM Usage (MB)\": peak_ram,\n",
        "            \"Total Inference Time (s)\": inf_time,\n",
        "            \"Average Inference Time per Sample (s)\": avg_inf_time,\n",
        "            \"MAE\": mae,\n",
        "            \"MSE\": mse,\n",
        "            \"R² Score\": r2,\n",
        "            \"Memory Usage (MB)\": memory_usage\n",
        "        }\n",
        "        print()\n",
        "    return results\n",
        "\n",
        "# Assuming X_train, y_train, X_test, y_test are already defined\n",
        "results = run_models(X_train, y_train, X_test, y_test)\n",
        "\n",
        "# Convert results dictionary into a DataFrame\n",
        "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
        "\n",
        "# Display results\n",
        "display(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMls_x86K4TM"
      },
      "source": [
        "Overall Table and Comparison Chart for ease of determining each Algorithm strengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWiIH18TBheY"
      },
      "outputs": [],
      "source": [
        "# Re-import necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure proper formatting of the dataset\n",
        "results_df = pd.DataFrame.from_dict(results, orient='index')  # Recreate DataFrame if needed\n",
        "results_df.reset_index(inplace=True)\n",
        "results_df.rename(columns={'index': 'Model'}, inplace=True)\n",
        "\n",
        "# Convert all numerical columns to float for consistency\n",
        "for col in results_df.columns[1:]:  # Exclude 'Model' column\n",
        "    results_df[col] = pd.to_numeric(results_df[col], errors='coerce')\n",
        "\n",
        "# Generate grouped bar charts for better readability\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(18, 12))\n",
        "fig.suptitle(\"Comparison of Model Performance Metrics\", fontsize=16)\n",
        "\n",
        "# Flatten axes for easy iteration\n",
        "axes = axes.flatten()\n",
        "\n",
        "# List of metrics for visualization\n",
        "metrics_to_plot = [\n",
        "    \"Training Time (s)\", \"Average CPU Usage (%)\", \"Peak RAM Usage (MB)\",\n",
        "    \"Total Inference Time (s)\", \"Average Inference Time per Sample (s)\",\n",
        "    \"MAE\", \"MSE\", \"R² Score\", \"Memory Usage (MB)\"\n",
        "]\n",
        "\n",
        "# Generate bar charts with correct hue assignment\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "    if metric in results_df.columns:  # Ensure metric exists in DataFrame\n",
        "        sns.barplot(data=results_df, x=\"Model\", y=metric, hue=\"Model\", dodge=False, ax=axes[i], palette=\"viridis\")\n",
        "        axes[i].set_title(metric, fontsize=12)\n",
        "        axes[i].set_xlabel(\"\")\n",
        "        axes[i].set_ylabel(metric)\n",
        "        axes[i].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Auto-scale y-axis based on data range\n",
        "        axes[i].set_ylim([0, results_df[metric].max() * 1.1])  # Add 10% padding\n",
        "\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j3fQpTRj5B0"
      },
      "source": [
        "#Step 4: Additional Features Extraction (Run Step 1 Before this)\n",
        "\n",
        "*   Run Everything\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voce0BPzsvXo"
      },
      "source": [
        "Copy Dataframe for Step 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUeDoVqm6ow_"
      },
      "outputs": [],
      "source": [
        "df_sd6 = df_sd3.copy()\n",
        "\n",
        "print('Original Data Frame:', df_sd3.shape)\n",
        "print('Copied Data Frame:', df_sd6.shape)\n",
        "print()\n",
        "\n",
        "print(df_sd6['JobTitle'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-I5hpOUxiz7"
      },
      "source": [
        "## Step 4.1: Additional Feature 1 ('Generation'), Clustering 'Age'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URV0eIoKrVW-"
      },
      "outputs": [],
      "source": [
        "# Define the reference year based on the dataset's last update\n",
        "reference_year = 2023\n",
        "\n",
        "# Calculate the Birth Year\n",
        "df_sd6['BirthYear'] = reference_year - df_sd6['Age']\n",
        "\n",
        "# Define bins and labels for generation categories\n",
        "bins = [0, 1943, 1964, 1979, 1994, float('inf')]\n",
        "labels = ['SilentGeneration', 'BabyBoomer', 'GenX', 'Millennials', 'GenZ']\n",
        "\n",
        "# Assign generations based on Birth Year\n",
        "df_sd6['Generation'] = pd.cut(df_sd6['BirthYear'], bins=bins, labels=labels, right=True)\n",
        "\n",
        "# Get the frequency of each label under 'Generation'\n",
        "generation_counts = df_sd6['Generation'].value_counts()\n",
        "\n",
        "# Print the frequency\n",
        "print(\"Frequency of each generation type:\")\n",
        "print(generation_counts)\n",
        "print('')\n",
        "\n",
        "# Create a pie chart\n",
        "plt.figure(figsize=(8, 8))  # Adjust figure size if needed\n",
        "# Remove categories with zero count\n",
        "filtered_counts = generation_counts[generation_counts > 0]\n",
        "\n",
        "plt.pie(filtered_counts, labels=filtered_counts.index,\n",
        "        autopct='%1.1f%%', startangle=90, textprops={'fontsize': 12})\n",
        "plt.title('Distribution of Generations')\n",
        "plt.show()\n",
        "print('')\n",
        "\n",
        "print(df_sd6.shape)\n",
        "print('')\n",
        "print(\"Updated Columns of Data Set\", df_sd6.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kpc9dcMkuqFo"
      },
      "source": [
        "## Step 4.2: Additional Feature 1 ('Seniority'), Clustering 'YrsExp'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWiZXhQwvDgq"
      },
      "outputs": [],
      "source": [
        "def assign_seniority(years):\n",
        "    if years <= 3:\n",
        "        return \"Entry\"\n",
        "    elif 4 <= years <= 6:\n",
        "        return \"Junior\"\n",
        "    elif 7 <= years <= 10:\n",
        "        return \"Mid\"\n",
        "    elif 11 <= years <= 15:\n",
        "        return \"Senior\"\n",
        "    else:\n",
        "        return \"Executive\"\n",
        "\n",
        "# Apply the mapping function to create the new 'Seniority' column\n",
        "df_sd6['Seniority'] = df_sd6['YrsExp'].apply(assign_seniority)\n",
        "\n",
        "# Get the frequency of each label under 'Generation'\n",
        "seniority_counts = df_sd6['Seniority'].value_counts()\n",
        "\n",
        "# Print the frequency\n",
        "print(\"Frequency of each Seniority:\")\n",
        "print(seniority_counts)\n",
        "print('')\n",
        "\n",
        "df_sd6.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifDCetaYxY-r"
      },
      "source": [
        "## Step 4.3: Additional Feature 3 ('Industry'), Clustering of 'JobTitle'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yma2Qq_EwlTW"
      },
      "source": [
        "* Imports relevant libaries\n",
        "* Cleans job titles (lowercase, removes special characters)\n",
        "* Tokenises job titles into words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-n92mi6ztZg"
      },
      "outputs": [],
      "source": [
        "!pip install gensim\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, pairwise_distances\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "from collections import Counter\n",
        "\n",
        "def preprocess_job_title(title):\n",
        "    title = title.lower()  # Convert to lowercase\n",
        "    title = re.sub(r'[^a-z\\s]', '', title)  # Remove special characters\n",
        "    return title.split()  # Tokenize by splitting on spaces\n",
        "\n",
        "# Apply preprocessing to job titles\n",
        "df_sd6[\"Cleaned_Job_Title\"] = df_sd6[\"JobTitle\"].astype(str).apply(preprocess_job_title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ab9YwxLwzbD"
      },
      "source": [
        "* Trains a Word2Vec model on job titles\n",
        "* Converts job titles into vector embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywyF2BfMz1a1"
      },
      "outputs": [],
      "source": [
        "# Step 2: Train Word2Vec for Job Title Embeddings\n",
        "word2vec_model = Word2Vec(sentences=df_sd6[\"Cleaned_Job_Title\"], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "def get_vector(title_tokens):\n",
        "    vectors = [word2vec_model.wv[word] for word in title_tokens if word in word2vec_model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(100)\n",
        "\n",
        "# Generate word embeddings\n",
        "df_sd6[\"Job_Title_Vector\"] = df_sd6[\"Cleaned_Job_Title\"].apply(get_vector)\n",
        "X = np.vstack(df_sd6[\"Job_Title_Vector\"].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UqyVDL0xA2t"
      },
      "source": [
        "* Uses Elbow Method (inertia) and Silhouette Score to find the best number of clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yLH8KZWz4Vl"
      },
      "outputs": [],
      "source": [
        "# Step 3: Determine the Optimal Number of Clusters\n",
        "inertia = []\n",
        "silhouette_scores = []\n",
        "cluster_range = range(2, 11)\n",
        "\n",
        "for k in cluster_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    cluster_labels = kmeans.fit_predict(X)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "    silhouette_scores.append(silhouette_score(X, cluster_labels))\n",
        "\n",
        "# Plot Elbow & Silhouette Score Side-by-Side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "axes[0].plot(cluster_range, inertia, marker='o', linestyle='-', label='Inertia')\n",
        "axes[0].set_xlabel(\"Number of Clusters\")\n",
        "axes[0].set_ylabel(\"Inertia\")\n",
        "axes[0].set_title(\"Elbow Method for Optimal Clusters\")\n",
        "axes[0].legend()\n",
        "axes[0].grid()\n",
        "\n",
        "axes[1].plot(cluster_range, silhouette_scores, marker='o', linestyle='-', label='Silhouette Score', color='red')\n",
        "axes[1].set_xlabel(\"Number of Clusters\")\n",
        "axes[1].set_ylabel(\"Silhouette Score\")\n",
        "axes[1].set_title(\"Silhouette Score for Cluster Quality\")\n",
        "axes[1].legend()\n",
        "axes[1].grid()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz9TW5vJxE93"
      },
      "source": [
        "* Selects the best cluster count based on the highest Silhouette Score\n",
        "* Assigns job titles to clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avO1wG2sz74A"
      },
      "outputs": [],
      "source": [
        "# Step 4: Apply K-Means Clustering Using Optimal Cluster Count\n",
        "optimal_clusters = cluster_range[silhouette_scores.index(max(silhouette_scores))]  # Best Silhouette Score\n",
        "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42, n_init=10)\n",
        "df_sd6[\"Cluster\"] = kmeans.fit_predict(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTiOi0VOxLSd"
      },
      "source": [
        "* Finds the most common words in each cluster to infer industry labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-zuZvyCz-gz"
      },
      "outputs": [],
      "source": [
        "# Step 5: Extract Common Words from Each Cluster to Infer Industry Labels\n",
        "def get_most_common_words(cluster_number):\n",
        "    titles_in_cluster = df_sd6[df_sd6[\"Cluster\"] == cluster_number][\"Cleaned_Job_Title\"].tolist()\n",
        "    all_words = [word for title in titles_in_cluster for word in title]\n",
        "    most_common_words = [word for word, _ in Counter(all_words).most_common(3)]\n",
        "    return most_common_words\n",
        "\n",
        "# Assign inferred industry names based on most common words\n",
        "industry_names = {}\n",
        "for cluster in range(optimal_clusters):\n",
        "    common_words = get_most_common_words(cluster)\n",
        "    industry_names[cluster] = \"-\".join(common_words)  # Use common words as a proxy for industry\n",
        "\n",
        "df_sd6[\"Industry\"] = df_sd6[\"Cluster\"].map(industry_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VOKAgfWxOZ5"
      },
      "source": [
        "* Reduces job title embeddings to 2D using t-SNE for visualization\n",
        "* Colours each job title by its assigned cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gISfajsi0ALR"
      },
      "outputs": [],
      "source": [
        "# Step 6: Visualize t-SNE Representation\n",
        "tsne = TSNE(n_components=2, perplexity=15, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=df_sd6[\"Cluster\"], cmap='tab10', alpha=0.7)\n",
        "plt.colorbar(scatter, label=\"Cluster\")\n",
        "plt.title(\"t-SNE Visualization of Job Titles (Coloured by Cluster)\")\n",
        "plt.xlabel(\"t-SNE Dimension 1\")\n",
        "plt.ylabel(\"t-SNE Dimension 2\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh93XRCgxT8b"
      },
      "source": [
        "* Creates a hierarchical clustering dendrogram for job titles\n",
        "* Colors clusters and adds a legend dynamically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3JqPI2O0CwF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# Generate Dendrogram\n",
        "linkage_matrix = linkage(X, method='ward')\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "dendro = dendrogram(\n",
        "    linkage_matrix,\n",
        "    labels=df_sd6[\"JobTitle\"].values,\n",
        "    leaf_rotation=45,\n",
        "    leaf_font_size=8,\n",
        "    truncate_mode='lastp',\n",
        "    p=20,\n",
        "    color_threshold=0.5 * max(linkage_matrix[:, 2])\n",
        ")\n",
        "\n",
        "# Extract the actual colors used\n",
        "cluster_colors = set(dendro['leaves_color_list'])\n",
        "legend_patches = [mpatches.Patch(color=color, label=f\"Cluster {i+1}\") for i, color in enumerate(cluster_colors)]\n",
        "\n",
        "# Add a legend\n",
        "plt.legend(handles=legend_patches, title=\"Clusters\", loc='upper right')\n",
        "\n",
        "plt.title(\"Hierarchical Clustering Dendrogram for Job Titles (With Correct Legend)\")\n",
        "plt.xlabel(\"Job Title Clusters\")\n",
        "plt.ylabel(\"Cluster Distance\")\n",
        "plt.show()\n",
        "\n",
        "print()\n",
        "df_sd6['Industry'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgDPLO4RQkOA"
      },
      "source": [
        "## Step 4.4: Heatmap for new categorical features relationships of Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iwc4OFMlPBzm"
      },
      "outputs": [],
      "source": [
        "def cramers_v(confusion_matrix):\n",
        "    \"\"\"Calculate Cramér's V statistic for association between two categorical variables\"\"\"\n",
        "    chi2 = stats.chi2_contingency(confusion_matrix)[0]  # Get Chi-square value\n",
        "    n = confusion_matrix.sum().sum()  # Total observations\n",
        "    k = min(confusion_matrix.shape)  # Minimum of row/column count\n",
        "    return np.sqrt(chi2 / (n * (k - 1)))\n",
        "\n",
        "# List of categorical variables\n",
        "new_categorical_vars = ['Gender', 'EduLevel', 'JobTitle', 'Industry', 'Generation', 'Seniority']\n",
        "\n",
        "# Compute Cramér's V matrix\n",
        "cramers_v_matrix = pd.DataFrame(np.zeros((len(new_categorical_vars), len(new_categorical_vars))),\n",
        "                                index=new_categorical_vars, columns=new_categorical_vars)\n",
        "\n",
        "for i, var1 in enumerate(new_categorical_vars):\n",
        "    for j, var2 in enumerate(new_categorical_vars):\n",
        "        if i == j:\n",
        "            cramers_v_matrix.iloc[i, j] = 1.0  # Diagonal should be 1 (same variable)\n",
        "        else:\n",
        "            contingency_table = pd.crosstab(df_sd6[var1], df_sd6[var2])\n",
        "            cramers_v_matrix.iloc[i, j] = cramers_v(contingency_table)\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cramers_v_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Cramér's V Heatmap for Categorical Variables\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0MrqPYYaf9f"
      },
      "source": [
        "# Step 5: Determine if varying Additional Features Combinations brings improvement to the Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E4ut4URahas"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import psutil\n",
        "import tracemalloc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "def measure_training_time(model, X_train, y_train):\n",
        "    cpu_before = psutil.cpu_percent(interval=None)\n",
        "    tracemalloc.start()\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    end_time = time.time()\n",
        "    cpu_after = psutil.cpu_percent(interval=None)\n",
        "    current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    peak_ram = peak_memory / (1024 * 1024)  # Convert to MB\n",
        "    training_time = end_time - start_time\n",
        "    avg_cpu = (cpu_before + cpu_after) / 2\n",
        "\n",
        "    return training_time, avg_cpu, peak_ram\n",
        "\n",
        "def measure_inference_time(model, X_test):\n",
        "    start_time = time.time()\n",
        "    y_pred = model.predict(X_test)\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    avg_time = total_time / len(X_test)\n",
        "    return total_time, avg_time, y_pred\n",
        "\n",
        "def measure_memory_usage():\n",
        "    process = psutil.Process()\n",
        "    memory_usage = process.memory_info().rss / (1024 * 1024)\n",
        "    return memory_usage\n",
        "\n",
        "def evaluate_feature_set(feature_set_name, df, categorical_ordinal, categorical_nominal, numerical_cols):\n",
        "    df_sd7 = df.drop(columns=['JobTitle', 'BirthYear', 'Cleaned_Job_Title', 'Job_Title_Vector', 'Cluster'])\n",
        "\n",
        "    X = df_sd7.drop(columns=['Salary'])\n",
        "    y = df_sd7['Salary']\n",
        "\n",
        "    for col in categorical_ordinal:\n",
        "        le = LabelEncoder()\n",
        "        X[col] = le.fit_transform(X[col])\n",
        "\n",
        "    ohe = OneHotEncoder(sparse_output=False)\n",
        "    X_encoded = ohe.fit_transform(X[categorical_nominal])\n",
        "    encoded_feature_names = ohe.get_feature_names_out(categorical_nominal)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X[numerical_cols])\n",
        "\n",
        "    X_processed = np.hstack((X_scaled, X[categorical_ordinal].values, X_encoded))\n",
        "    X_final = pd.DataFrame(X_processed, columns=numerical_cols + categorical_ordinal + list(encoded_feature_names))\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for model_name, model in zip([\"Random Forest\", \"XGBoost\"], [RandomForestRegressor(random_state=42), XGBRegressor(n_estimators=200, learning_rate=0.05, random_state=42)]):\n",
        "        train_time, avg_cpu, peak_ram = measure_training_time(model, X_train, y_train)\n",
        "        inf_time, avg_inf_time, y_pred = measure_inference_time(model, X_test)\n",
        "        memory_usage = measure_memory_usage()\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        results.append({\n",
        "            \"Feature Set\": feature_set_name,\n",
        "            \"Model\": model_name,\n",
        "            \"Training Time (s)\": train_time,\n",
        "            \"Average CPU Usage (%)\": avg_cpu,\n",
        "            \"Peak RAM Usage (MB)\": peak_ram,\n",
        "            \"Total Inference Time (s)\": inf_time,\n",
        "            \"Average Inference Time per Sample (s)\": avg_inf_time,\n",
        "            \"MAE\": mae,\n",
        "            \"MSE\": mse,\n",
        "            \"R² Score\": r2,\n",
        "            \"Memory Usage (MB)\": memory_usage\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "feature_combinations = [\n",
        "    (\"Baseline (No Additional Features)\", [\"EduLevel\"], [\"Gender\"], [\"Age\", \"YrsExp\"]),\n",
        "    (\"Generation\", [\"EduLevel\", \"Generation\"], [\"Gender\"], [\"Age\", \"YrsExp\"]),\n",
        "    (\"Seniority\", [\"EduLevel\", \"Seniority\"], [\"Gender\"], [\"Age\", \"YrsExp\"]),\n",
        "    (\"Generation + Seniority\", [\"EduLevel\", \"Generation\", \"Seniority\"], [\"Gender\"], [\"Age\", \"YrsExp\"]),\n",
        "    (\"Industry\", [\"EduLevel\"], [\"Gender\", \"Industry\"], [\"Age\", \"YrsExp\"]),\n",
        "    (\"Generation + Industry\", [\"EduLevel\", \"Generation\"], [\"Gender\", \"Industry\"], [\"Age\", \"YrsExp\"]),\n",
        "    (\"Seniority + Industry\", [\"EduLevel\", \"Seniority\"], [\"Gender\", \"Industry\"], [\"Age\", \"YrsExp\"]),\n",
        "    (\"All Features\", [\"EduLevel\", \"Generation\", \"Seniority\"], [\"Gender\", \"Industry\"], [\"Age\", \"YrsExp\"])\n",
        "]\n",
        "\n",
        "results_df = pd.DataFrame()\n",
        "for feature_name, cat_ord, cat_nom, num_cols in feature_combinations:\n",
        "    results_df = pd.concat([results_df, evaluate_feature_set(feature_name, df_sd6, cat_ord, cat_nom, num_cols)])\n",
        "\n",
        "display(results_df)\n",
        "print()\n",
        "display(results_df[results_df['Model']=='Random Forest'])\n",
        "print()\n",
        "display(results_df[results_df['Model']=='XGBoost'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfM1IBU1kBKn"
      },
      "source": [
        "#Step 6: Machine Learning Algorithm Implementation with Additional Features (Run Step 4 Before this)\n",
        "\n",
        "1. We will encode the relevant columns of data that fit our problem statement, this includes our new Additional Features\n",
        "2. Next, we will spilt the Dataset\n",
        "3. Lastly, we will run the best 2 Machine Learning Algorithm we have identified in Step 3 to see if the model improves\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8VwNdJvSTDH"
      },
      "source": [
        "## Step 6.1: Encode and Scale all Three Additional Features ('Generation'+'Seniority'+'Industry')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3Q4G7lmn8C2"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
        "\n",
        "# Drop 'JobTitle' (high cardinality, too specific)\n",
        "df_sd8 = df_sd6.drop(columns=['JobTitle', 'BirthYear', 'Cleaned_Job_Title', 'Job_Title_Vector', 'Cluster'])\n",
        "\n",
        "# Separate features and target\n",
        "X = df_sd8.drop(columns=['Salary'])\n",
        "y = df_sd8['Salary']\n",
        "\n",
        "# Identify feature types\n",
        "categorical_ordinal = ['EduLevel', 'Generation', 'Seniority']  # Ordinal categories\n",
        "categorical_nominal = ['Gender', 'Industry']  # Nominal categories\n",
        "numerical_cols = ['Age', 'YrsExp']\n",
        "\n",
        "# Apply Label Encoding to ordinal categorical variables\n",
        "for col in categorical_ordinal:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Apply OneHotEncoding to nominal categorical variables\n",
        "ohe = OneHotEncoder(sparse_output=False)  # Avoid dummy variable trap\n",
        "X_encoded = ohe.fit_transform(X[categorical_nominal])\n",
        "encoded_feature_names = ohe.get_feature_names_out(categorical_nominal)\n",
        "\n",
        "# Scale numerical variables\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X[numerical_cols])\n",
        "\n",
        "# Combine all transformed features\n",
        "X_processed = np.hstack((X_scaled, X[categorical_ordinal].values, X_encoded))\n",
        "\n",
        "# Convert to DataFrame for clarity\n",
        "X_final = pd.DataFrame(X_processed, columns=numerical_cols + categorical_ordinal + list(encoded_feature_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJxVXqEKPz8f"
      },
      "source": [
        "## Step 6.2: Machine Learning Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYsyamil2EGe"
      },
      "source": [
        "Spilting of Training Data (Must Run before Algorithm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jutDOYBp2Hiy"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Train-test split (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZUhNvkU2Q6m"
      },
      "source": [
        "Algorithm 3: RandomForestRegressor (Train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONoa5t0vw7jL"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "# rfr_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rfr_model = RandomForestRegressor(random_state=42)\n",
        "rfr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict salaries on the test set\n",
        "y_pred_rfr = rfr_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "mae_rfr = mean_absolute_error(y_test,y_pred_rfr)\n",
        "mse_rfr = mean_squared_error(y_test, y_pred_rfr)    # Mean Squared Error\n",
        "r2_rfr = r2_score(y_test, y_pred_rfr)\n",
        "\n",
        "# Print results\n",
        "print(\"Random Forest Regressor Performance:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_rfr:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_rfr:.2f}\")\n",
        "print(f\"R² Score: {r2_rfr:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na1fJ-QtY5sR"
      },
      "source": [
        "Algorithm 4: XGBoost (Train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egbcclyJY71s"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "xgb_model = XGBRegressor(n_estimators=200, learning_rate=0.05, random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict salaries on the test set\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost Performance\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_xgb:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_xgb:.2f}\")\n",
        "print(f\"R² Score: {r2_xgb:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT80b-AVIrC1"
      },
      "source": [
        "# Step 7: Fine-Tunning Machine Learning Algorithm (Run Step 6 Before this)\n",
        "\n",
        "*   Pick either Exhaustive or Faster\n",
        "*   According to your Algorithm of your Choice\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpWA9oOCIXpA"
      },
      "source": [
        "## Step 7.1: (Choose 1 of 2): Fine-Tuning with Grid Search (Exhaustive)\n",
        "*   Grid Search tests all possible combinations of specified hyperparameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9WDCD1-IMDF"
      },
      "source": [
        "**Algorithm 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmEZm1eTH7Nc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "fine_tune_choice = 1\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid_rfr = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Perform Grid Search\n",
        "grid_search_rfr = GridSearchCV(rfr_model, param_grid_rfr, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=2)\n",
        "grid_search_rfr.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and performance\n",
        "print(\"Best Parameters:\", grid_search_rfr.best_params_)\n",
        "print(\"Best MAE:\", -grid_search_rfr.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cboUUkPvIQsr"
      },
      "source": [
        "**Algorithm 4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZBuPPD3ITko"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "fine_tune_choice = 1\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'min_child_weight': [1, 3, 5],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "# Grid Search\n",
        "grid_search_xgb = GridSearchCV(xgb_model, param_grid_xgb, scoring='neg_mean_absolute_error', cv=5, verbose=2, n_jobs=-1)\n",
        "grid_search_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Best Parameters\n",
        "print(\"Best Parameters:\", grid_search_xgb.best_params_)\n",
        "print(\"Best MAE:\", -grid_search_xgb.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkKbl-XNJNOU"
      },
      "source": [
        "## Step 7.1: (Choose 2 of 2): Fine-Tuning with Random Search (Faster)\n",
        "*   Random Search randomly samples hyperparameters instead of testing all combinations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxJpGrjaIifY"
      },
      "source": [
        " **Algorithm 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvoRUi9vJT8g"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "fine_tune_choice = 2\n",
        "\n",
        "# Define the parameter grid with distributions\n",
        "param_dist_rfr = {\n",
        "    'n_estimators': randint(100, 500),\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': randint(2, 20),\n",
        "    'min_samples_leaf': randint(1, 10),\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Perform Randomized Search\n",
        "random_search_rfr = RandomizedSearchCV(rfr_model, param_dist_rfr, n_iter=20, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=2, random_state=42)\n",
        "random_search_rfr.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and performance\n",
        "print(\"Best Parameters:\", random_search_rfr.best_params_)\n",
        "print(\"Best MAE:\", -random_search_rfr.best_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh_lJPVXoCmO"
      },
      "source": [
        " **Algorithm 4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlZK-r1uoCyq"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "fine_tune_choice = 2\n",
        "\n",
        "# Define the parameter distribution\n",
        "param_dist_xgb = {\n",
        "    'n_estimators': randint(100, 500),\n",
        "    'learning_rate': uniform(0.01, 0.3),\n",
        "    'max_depth': randint(3, 10),\n",
        "    'min_child_weight': randint(1, 10),\n",
        "    'subsample': uniform(0.7, 0.3),\n",
        "    'colsample_bytree': uniform(0.7, 0.3),\n",
        "    'gamma': uniform(0, 0.5),\n",
        "    'reg_alpha': uniform(0, 0.5),\n",
        "    'reg_lambda': uniform(0, 0.5)\n",
        "}\n",
        "\n",
        "# Randomized Search\n",
        "random_search_xgb = RandomizedSearchCV(xgb_model, param_dist_xgb, n_iter=20, scoring='neg_mean_absolute_error', cv=5, verbose=2, n_jobs=-1, random_state=42)\n",
        "random_search_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Best Parameters\n",
        "print(\"Best Parameters:\", random_search_xgb.best_params_)\n",
        "print(\"Best MAE:\", -random_search_xgb.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSE5Dow3LHVU"
      },
      "source": [
        "# Step 8: Retraining of Both Algorithm Models (Run only after Step 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu4LI4__S7cJ"
      },
      "source": [
        "## Step 8.1: Retrain (Algorithm 3: RandomForest) & (Algorithm 4: XGBoost)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8xA5j2xS7cK"
      },
      "source": [
        "**Algorithm 3**: Random Forest RegressorRetrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6yYTjsOS7cK"
      },
      "outputs": [],
      "source": [
        "if fine_tune_choice == 1:\n",
        "  best_params = grid_search_rfr.best_params_\n",
        "elif fine_tune_choice == 2:\n",
        "  best_params = random_search_rfr.best_params_\n",
        "\n",
        "best_model_rfr = RandomForestRegressor(**best_params, random_state=42)\n",
        "best_model_rfr.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_rfr = best_model_rfr.predict(X_test)\n",
        "\n",
        "mae_tuned = mean_absolute_error(y_test, y_pred_rfr)\n",
        "mse_tuned = mean_squared_error(y_test, y_pred_rfr)\n",
        "r2_tuned = r2_score(y_test, y_pred_rfr)\n",
        "\n",
        "print(f\"Tuned Model MAE: {mae_tuned:.2f}\")\n",
        "print(f\"Tuned Model MSE: {mse_tuned:.2f}\")\n",
        "print(f\"Tuned Model R² Score: {r2_tuned:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithm 4**: XGBoost"
      ],
      "metadata": {
        "id": "tM9XpNIlS7cL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fine_tune_choice == 1:\n",
        "  best_params = grid_search_xgb.best_params_\n",
        "elif fine_tune_choice == 2:\n",
        "  best_params = random_search_xgb.best_params_\n",
        "\n",
        "best_model_xgb = XGBRegressor(**best_params, random_state=42)\n",
        "best_model_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_xgb = best_model_xgb.predict(X_test)\n",
        "\n",
        "mae_tuned = mean_absolute_error(y_test, y_pred_xgb)\n",
        "mse_tuned = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_tuned = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"Tuned Model MAE: {mae_tuned:.2f}\")\n",
        "print(f\"Tuned Model MSE: {mse_tuned:.2f}\")\n",
        "print(f\"Tuned Model R² Score: {r2_tuned:.4f}\")"
      ],
      "metadata": {
        "id": "WN2yzgz9S7cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiZ3aUYJtsXk"
      },
      "source": [
        "## Step 8.2: Comparing Both Metrics Before & After Tuning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import psutil\n",
        "import tracemalloc\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def measure_training_time(model, X_train, y_train):\n",
        "    cpu_before = psutil.cpu_percent(interval=None)\n",
        "    tracemalloc.start()\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    end_time = time.time()\n",
        "    cpu_after = psutil.cpu_percent(interval=None)\n",
        "    current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    peak_ram = peak_memory / (1024 * 1024)  # Convert to MB\n",
        "    training_time = end_time - start_time\n",
        "    avg_cpu = (cpu_before + cpu_after) / 2\n",
        "\n",
        "    print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "    print(f\"Average CPU Usage: {avg_cpu:.2f}%\")\n",
        "    print(f\"Peak RAM Usage: {peak_ram:.2f} MB\")\n",
        "\n",
        "    return training_time, avg_cpu, peak_ram\n",
        "\n",
        "def measure_inference_time(model, X_test):\n",
        "    start_time = time.time()\n",
        "    y_pred = model.predict(X_test)\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    avg_time = total_time / len(X_test)\n",
        "    print(f\"Total Inference Time: {total_time:.2f} seconds\")\n",
        "    print(f\"Average Inference Time per Sample: {avg_time:.6f} seconds\")\n",
        "    return total_time, avg_time, y_pred\n",
        "\n",
        "def measure_memory_usage():\n",
        "    process = psutil.Process()\n",
        "    memory_usage = process.memory_info().rss / (1024 * 1024)\n",
        "    print(f\"Memory Usage: {memory_usage:.2f} MB\")\n",
        "    return memory_usage\n",
        "\n",
        "def evaluate_model(y_test, y_pred, model_name):\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(f\"{model_name} Performance:\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "    return mae, mse, r2\n",
        "\n",
        "\n",
        "### 📌 Compare **RandomForestRegressor** Before & After Tuning\n",
        "\n",
        "print(\"\\n--- Original Random Forest Model Performance ---\")\n",
        "train_time_orig, avg_cpu_orig, peak_ram_orig = measure_training_time(rfr_model, X_train, y_train)\n",
        "inf_time_orig, avg_inf_time_orig, y_pred_orig = measure_inference_time(rfr_model, X_test)\n",
        "mae_orig, mse_orig, r2_orig = evaluate_model(y_test, y_pred_orig, \"Random Forest (Before Tune)\")\n",
        "memory_usage_orig = measure_memory_usage()\n",
        "\n",
        "print(\"\\n--- Fine-Tuned Random Forest Model Performance ---\")\n",
        "train_time_tuned, avg_cpu_tuned, peak_ram_tuned = measure_training_time(best_model_rfr, X_train, y_train)\n",
        "inf_time_tuned, avg_inf_time_tuned, y_pred_tuned = measure_inference_time(best_model_rfr, X_test)\n",
        "mae_tuned, mse_tuned, r2_tuned = evaluate_model(y_test, y_pred_tuned, \"Random Forest (After Tune)\")\n",
        "memory_usage_tuned = measure_memory_usage()\n",
        "\n",
        "# 📊 Create comparison table for Random Forest\n",
        "rf_comparison = {\n",
        "    \"Metric\": [\"Training Time (s)\", \"Avg CPU Usage (%)\", \"Peak RAM Usage (MB)\",\n",
        "               \"Total Inference Time (s)\", \"Avg Inference Time per Sample (s)\",\n",
        "               \"MAE\", \"MSE\", \"R² Score\", \"Memory Usage (MB)\"],\n",
        "    \"RandomForest_Before\": [train_time_orig, avg_cpu_orig, peak_ram_orig,\n",
        "                             inf_time_orig, avg_inf_time_orig,\n",
        "                             mae_orig, mse_orig, r2_orig, memory_usage_orig],\n",
        "    \"RandomForest_After\": [train_time_tuned, avg_cpu_tuned, peak_ram_tuned,\n",
        "                            inf_time_tuned, avg_inf_time_tuned,\n",
        "                            mae_tuned, mse_tuned, r2_tuned, memory_usage_tuned]\n",
        "}\n",
        "\n",
        "rf_comparison_df = pd.DataFrame(rf_comparison)\n",
        "display(rf_comparison_df)\n",
        "\n",
        "\n",
        "### 📌 Compare **XGBoost** Before & After Tuning\n",
        "\n",
        "print(\"\\n--- Original XGBoost Model Performance ---\")\n",
        "train_time_orig_xgb, avg_cpu_orig_xgb, peak_ram_orig_xgb = measure_training_time(xgb_model, X_train, y_train)\n",
        "inf_time_orig_xgb, avg_inf_time_orig_xgb, y_pred_orig_xgb = measure_inference_time(xgb_model, X_test)\n",
        "mae_orig_xgb, mse_orig_xgb, r2_orig_xgb = evaluate_model(y_test, y_pred_orig_xgb, \"XGBoost (Before Tune)\")\n",
        "memory_usage_orig_xgb = measure_memory_usage()\n",
        "\n",
        "print(\"\\n--- Fine-Tuned XGBoost Model Performance ---\")\n",
        "train_time_tuned_xgb, avg_cpu_tuned_xgb, peak_ram_tuned_xgb = measure_training_time(best_model_xgb, X_train, y_train)\n",
        "inf_time_tuned_xgb, avg_inf_time_tuned_xgb, y_pred_tuned_xgb = measure_inference_time(best_model_xgb, X_test)\n",
        "mae_tuned_xgb, mse_tuned_xgb, r2_tuned_xgb = evaluate_model(y_test, y_pred_tuned_xgb, \"XGBoost (After Tune)\")\n",
        "memory_usage_tuned_xgb = measure_memory_usage()\n",
        "\n",
        "# 📊 Create comparison table for XGBoost\n",
        "xgb_comparison = {\n",
        "    \"Metric\": [\"Training Time (s)\", \"Avg CPU Usage (%)\", \"Peak RAM Usage (MB)\",\n",
        "               \"Total Inference Time (s)\", \"Avg Inference Time per Sample (s)\",\n",
        "               \"MAE\", \"MSE\", \"R² Score\", \"Memory Usage (MB)\"],\n",
        "    \"XGBoost_Before\": [train_time_orig_xgb, avg_cpu_orig_xgb, peak_ram_orig_xgb,\n",
        "                        inf_time_orig_xgb, avg_inf_time_orig_xgb,\n",
        "                        mae_orig_xgb, mse_orig_xgb, r2_orig_xgb, memory_usage_orig_xgb],\n",
        "    \"XGBoost_After\": [train_time_tuned_xgb, avg_cpu_tuned_xgb, peak_ram_tuned_xgb,\n",
        "                       inf_time_tuned_xgb, avg_inf_time_tuned_xgb,\n",
        "                       mae_tuned_xgb, mse_tuned_xgb, r2_tuned_xgb, memory_usage_tuned_xgb]\n",
        "}\n",
        "\n",
        "xgb_comparison_df = pd.DataFrame(xgb_comparison)\n",
        "display(xgb_comparison_df)"
      ],
      "metadata": {
        "id": "DJO1Cwu-O1TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8.3: Comparing Both Feature Importance Before & After Tuning"
      ],
      "metadata": {
        "id": "XLFmT4gBSkWU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t44grrpqvo2r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Feature importances for Random Forest models\n",
        "feature_importances_rfr_before = rfr_model.feature_importances_\n",
        "feature_importances_rfr_after = best_model_rfr.feature_importances_\n",
        "# Feature importances for XGBoost models\n",
        "feature_importances_xgb_before = xgb_model.feature_importances_\n",
        "feature_importances_xgb_after = best_model_xgb.feature_importances_\n",
        "\n",
        "feature_names = X_final.columns\n",
        "\n",
        "# Sorting indices\n",
        "sorted_idx_rfr_before = np.argsort(feature_importances_rfr_before)[::-1]\n",
        "sorted_idx_rfr_after = np.argsort(feature_importances_rfr_after)[::-1]\n",
        "sorted_idx_xgb_before = np.argsort(feature_importances_xgb_before)[::-1]\n",
        "sorted_idx_xgb_after = np.argsort(feature_importances_xgb_after)[::-1]\n",
        "\n",
        "# Create subplots for both models\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))  # 2 rows, 2 columns\n",
        "\n",
        "# Plot feature importances for Random Forest (Before Tuning)\n",
        "axes[0, 0].bar(range(len(feature_importances_rfr_before)),\n",
        "               feature_importances_rfr_before[sorted_idx_rfr_before],\n",
        "               tick_label=np.array(feature_names)[sorted_idx_rfr_before])\n",
        "axes[0, 0].set_xticklabels(np.array(feature_names)[sorted_idx_rfr_before], rotation=90)\n",
        "axes[0, 0].set_xlabel(\"Feature\")\n",
        "axes[0, 0].set_ylabel(\"Importance\")\n",
        "axes[0, 0].set_title(\"Feature Importance in Random Forest (Before Tune)\")\n",
        "\n",
        "# Plot feature importances for Random Forest (After Tuning)\n",
        "axes[0, 1].bar(range(len(feature_importances_rfr_after)),\n",
        "               feature_importances_rfr_after[sorted_idx_rfr_after],\n",
        "               tick_label=np.array(feature_names)[sorted_idx_rfr_after])\n",
        "axes[0, 1].set_xticklabels(np.array(feature_names)[sorted_idx_rfr_after], rotation=90)\n",
        "axes[0, 1].set_xlabel(\"Feature\")\n",
        "axes[0, 1].set_ylabel(\"Importance\")\n",
        "axes[0, 1].set_title(\"Feature Importance in Random Forest (After Tune)\")\n",
        "\n",
        "# Plot feature importances for XGBoost (Before Tuning)\n",
        "axes[1, 0].bar(range(len(feature_importances_xgb_before)),\n",
        "               feature_importances_xgb_before[sorted_idx_xgb_before],\n",
        "               tick_label=np.array(feature_names)[sorted_idx_xgb_before])\n",
        "axes[1, 0].set_xticklabels(np.array(feature_names)[sorted_idx_xgb_before], rotation=90)\n",
        "axes[1, 0].set_xlabel(\"Feature\")\n",
        "axes[1, 0].set_ylabel(\"Importance\")\n",
        "axes[1, 0].set_title(\"Feature Importance in XGBRegressor (Before Tune)\")\n",
        "\n",
        "# Plot feature importances for XGBoost (After Tuning)\n",
        "axes[1, 1].bar(range(len(feature_importances_xgb_after)),\n",
        "               feature_importances_xgb_after[sorted_idx_xgb_after],\n",
        "               tick_label=np.array(feature_names)[sorted_idx_xgb_after])\n",
        "axes[1, 1].set_xticklabels(np.array(feature_names)[sorted_idx_xgb_after], rotation=90)\n",
        "axes[1, 1].set_xlabel(\"Feature\")\n",
        "axes[1, 1].set_ylabel(\"Importance\")\n",
        "axes[1, 1].set_title(\"Feature Importance in XGBRegressor (After Tune)\")\n",
        "\n",
        "# Adjust layout and display\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgwN-4S1xQIQ"
      },
      "source": [
        "# Step 9: Exploring Ensembling of Two Best Tuned Algorithms Models (Run only after Step 8)\n",
        "\n",
        "* We will explore whether if Ensembling is better than our Best Tuned Model\n",
        "* Conduct Performance Evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9.1: Three Ensemble Methods"
      ],
      "metadata": {
        "id": "buQ_6NDqYruT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI-PBsX6AWNe"
      },
      "source": [
        "(Ensemble 1): Adaptive Dynamic Weighted Averaging (Performance-Based Weights)\n",
        "* This code averages predictions from two models (Random Forest and XGBoost) using weighted averaging.\n",
        "\n",
        "* The weights are determined based on the R² scores of each model, meaning that models with higher performance (higher R² score) are given more weight.\n",
        "\n",
        "* Weights are normalised to the sum up to 1.\n",
        "\n",
        "Example: If XGBoost has an R² of 0.85 and Random Forest has an R² of 0.75, their weights would be:\n",
        "\n",
        "    w_rfr = 0.75 / (0.75 + 0.85) = 0.47\n",
        "    w_xgb = 0.85 / (0.75 + 0.85) = 0.53"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QGs3z_wHVTD"
      },
      "outputs": [],
      "source": [
        "# Define Weights Based on Performance (higher R² → higher weight)\n",
        "w_rfr = r2_score(y_test, y_pred_rfr)\n",
        "w_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "# Normalize Weights\n",
        "total_weight = w_rfr + w_xgb\n",
        "w_rfr /= total_weight\n",
        "w_xgb /= total_weight\n",
        "\n",
        "# Weighted Average\n",
        "y_pred_weighted = (w_rfr * y_pred_rfr) + (w_xgb * y_pred_xgb)\n",
        "\n",
        "# Evaluate Performance\n",
        "mae = mean_absolute_error(y_test, y_pred_weighted)\n",
        "mse = mean_squared_error(y_test, y_pred_weighted)\n",
        "r2 = r2_score(y_test, y_pred_weighted)\n",
        "\n",
        "print(\"Ensemble Model Performance (Weighted Averaging):\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cjU4rrLDf-v"
      },
      "source": [
        "(Ensemble 2): Static Weighted Averaging (Fixed Weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6m085IkDlnv"
      },
      "outputs": [],
      "source": [
        "# Define weights (adjust based on model performance)\n",
        "w_rfr = 0.4  # Adjust weight for Random Forest\n",
        "w_xgb = 0.6  # Adjust weight for XGBoost\n",
        "\n",
        "# Weighted ensemble\n",
        "y_pred_ensemble = (w_rfr * y_pred_rfr) + (w_xgb * y_pred_xgb)\n",
        "\n",
        "# Evaluate\n",
        "mae_ensemble = mean_absolute_error(y_test, y_pred_ensemble)\n",
        "mse_ensemble = mean_squared_error(y_test, y_pred_ensemble)\n",
        "r2_ensemble = r2_score(y_test, y_pred_ensemble)\n",
        "\n",
        "print(\"Weighted Ensemble Model:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_ensemble:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE):: {mse_ensemble:.2f}\")\n",
        "print(f\"R² Score: {r2_ensemble:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78DSA8fQAnzi"
      },
      "source": [
        "(Ensemble 3): Stacking Ensemble (Complex)\n",
        "\n",
        "* The predictions from Random Forest and XGBoost (base models) are used as features for a meta-model (Linear Regression).\n",
        "\n",
        "* The meta-model learns how to combine base model predictions to make a final prediction.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y70e36RPGtX4"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "\n",
        "# Extract the best parameters from tuning\n",
        "rf_best_params = {k: v for k, v in best_params.items() if k in ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'max_features']}\n",
        "xgb_best_params = {k: v for k, v in best_params.items() if k in ['n_estimators', 'learning_rate', 'max_depth', 'min_child_weight', 'subsample', 'colsample_bytree']}\n",
        "\n",
        "# Define Base Models (Using Correct Parameters for Each Model)\n",
        "base_models = [\n",
        "    ('rf', RandomForestRegressor(**rf_best_params, random_state=42)),\n",
        "    ('xgb', XGBRegressor(**xgb_best_params, random_state=42))\n",
        "]\n",
        "\n",
        "# Meta Model (Final Predictor)\n",
        "meta_model = LinearRegression()\n",
        "\n",
        "# Stacking Regressor\n",
        "stacking_reg = StackingRegressor(estimators=base_models, final_estimator=meta_model, cv=5)\n",
        "stacking_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on Test Data\n",
        "y_pred_stack = stacking_reg.predict(X_test)\n",
        "\n",
        "# Evaluate Performance\n",
        "mae_stacked = mean_absolute_error(y_test, y_pred_stack)\n",
        "mse_stacked = mean_squared_error(y_test, y_pred_stack)\n",
        "r2_stacked = r2_score(y_test, y_pred_stack)\n",
        "\n",
        "print(\"Stacking Model Performance:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_stacked:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_stacked:.2f}\")\n",
        "print(f\"R² Score: {r2_stacked:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.2: Compare Metrics for the Three Ensembling Methods against the Tuned Individual Models"
      ],
      "metadata": {
        "id": "IL9lc9pdf0p-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import psutil\n",
        "import tracemalloc\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def measure_execution_time(func, *args, **kwargs):\n",
        "    \"\"\"Measure execution time, CPU usage, memory usage, and inference time of a function.\"\"\"\n",
        "    cpu_before = psutil.cpu_percent(interval=None)\n",
        "    tracemalloc.start()\n",
        "    start_time = time.time()\n",
        "\n",
        "    result = func(*args, **kwargs)  # Run the function\n",
        "\n",
        "    end_time = time.time()\n",
        "    cpu_after = psutil.cpu_percent(interval=None)\n",
        "    current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    peak_ram = peak_memory / (1024 * 1024)  # Convert bytes to MB\n",
        "    execution_time = end_time - start_time\n",
        "    avg_cpu = (cpu_before + cpu_after) / 2  # Approximate CPU usage\n",
        "\n",
        "    # Measure memory usage after execution\n",
        "    process = psutil.Process()\n",
        "    memory_usage = process.memory_info().rss / (1024 * 1024)  # Convert bytes to MB\n",
        "\n",
        "    return execution_time, avg_cpu, peak_ram, memory_usage, result\n",
        "\n",
        "# Tracking Metrics for the Ensemble Methods and Tuned Models\n",
        "comparison_results = {}\n",
        "\n",
        "# --- Tuned Random Forest ---\n",
        "exec_time, avg_cpu, peak_ram, memory_usage, y_pred_rfr = measure_execution_time(\n",
        "    lambda: best_model_rfr.predict(X_test)\n",
        ")\n",
        "\n",
        "# Measure inference time\n",
        "inf_time_start = time.time()\n",
        "y_pred_rfr = best_model_rfr.predict(X_test)\n",
        "inf_time_end = time.time()\n",
        "total_inference_time = inf_time_end - inf_time_start\n",
        "average_inference_time = total_inference_time / len(y_test)\n",
        "\n",
        "# Evaluate Performance\n",
        "mae_rfr = mean_absolute_error(y_test, y_pred_rfr)\n",
        "mse_rfr = mean_squared_error(y_test, y_pred_rfr)\n",
        "r2_rfr = r2_score(y_test, y_pred_rfr)\n",
        "\n",
        "comparison_results[\"Tuned Random Forest\"] = {\n",
        "    \"Execution Time (s)\": exec_time,\n",
        "    \"Average CPU Usage (%)\": avg_cpu,\n",
        "    \"Peak RAM Usage (MB)\": peak_ram,\n",
        "    \"Total Inference Time (s)\": total_inference_time,\n",
        "    \"Average Inference Time per Sample (s)\": average_inference_time,\n",
        "    \"MAE\": mae_rfr,\n",
        "    \"MSE\": mse_rfr,\n",
        "    \"R² Score\": r2_rfr,\n",
        "    \"Memory Usage (MB)\": memory_usage\n",
        "}\n",
        "\n",
        "# --- Tuned XGBoost ---\n",
        "exec_time, avg_cpu, peak_ram, memory_usage, y_pred_xgb = measure_execution_time(\n",
        "    lambda: best_model_xgb.predict(X_test)\n",
        ")\n",
        "\n",
        "# Measure inference time\n",
        "inf_time_start = time.time()\n",
        "y_pred_xgb = best_model_xgb.predict(X_test)\n",
        "inf_time_end = time.time()\n",
        "total_inference_time = inf_time_end - inf_time_start\n",
        "average_inference_time = total_inference_time / len(y_test)\n",
        "\n",
        "# Evaluate Performance\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "comparison_results[\"Tuned XGBoost\"] = {\n",
        "    \"Execution Time (s)\": exec_time,\n",
        "    \"Average CPU Usage (%)\": avg_cpu,\n",
        "    \"Peak RAM Usage (MB)\": peak_ram,\n",
        "    \"Total Inference Time (s)\": total_inference_time,\n",
        "    \"Average Inference Time per Sample (s)\": average_inference_time,\n",
        "    \"MAE\": mae_xgb,\n",
        "    \"MSE\": mse_xgb,\n",
        "    \"R² Score\": r2_xgb,\n",
        "    \"Memory Usage (MB)\": memory_usage\n",
        "}\n",
        "\n",
        "# --- Adaptive Dynamic Weighted Averaging ---\n",
        "exec_time, avg_cpu, peak_ram, memory_usage, y_pred_weighted = measure_execution_time(\n",
        "    lambda: (w_rfr * y_pred_rfr) + (w_xgb * y_pred_xgb)\n",
        ")\n",
        "\n",
        "# Measure inference time\n",
        "inf_time_start = time.time()\n",
        "y_pred_weighted = (w_rfr * y_pred_rfr) + (w_xgb * y_pred_xgb)\n",
        "inf_time_end = time.time()\n",
        "total_inference_time = inf_time_end - inf_time_start\n",
        "average_inference_time = total_inference_time / len(y_test)\n",
        "\n",
        "# Evaluate Performance\n",
        "mae_weighted = mean_absolute_error(y_test, y_pred_weighted)\n",
        "mse_weighted = mean_squared_error(y_test, y_pred_weighted)\n",
        "r2_weighted = r2_score(y_test, y_pred_weighted)\n",
        "\n",
        "comparison_results[\"Adaptive Weighted\"] = {\n",
        "    \"Execution Time (s)\": exec_time,\n",
        "    \"Average CPU Usage (%)\": avg_cpu,\n",
        "    \"Peak RAM Usage (MB)\": peak_ram,\n",
        "    \"Total Inference Time (s)\": total_inference_time,\n",
        "    \"Average Inference Time per Sample (s)\": average_inference_time,\n",
        "    \"MAE\": mae_weighted,\n",
        "    \"MSE\": mse_weighted,\n",
        "    \"R² Score\": r2_weighted,\n",
        "    \"Memory Usage (MB)\": memory_usage\n",
        "}\n",
        "\n",
        "# --- Static Weighted Averaging ---\n",
        "exec_time, avg_cpu, peak_ram, memory_usage, y_pred_ensemble = measure_execution_time(\n",
        "    lambda: (0.4 * y_pred_rfr) + (0.6 * y_pred_xgb)\n",
        ")\n",
        "\n",
        "# Measure inference time\n",
        "inf_time_start = time.time()\n",
        "y_pred_ensemble = (0.4 * y_pred_rfr) + (0.6 * y_pred_xgb)\n",
        "inf_time_end = time.time()\n",
        "total_inference_time = inf_time_end - inf_time_start\n",
        "average_inference_time = total_inference_time / len(y_test)\n",
        "\n",
        "# Evaluate Performance\n",
        "mae_ensemble = mean_absolute_error(y_test, y_pred_ensemble)\n",
        "mse_ensemble = mean_squared_error(y_test, y_pred_ensemble)\n",
        "r2_ensemble = r2_score(y_test, y_pred_ensemble)\n",
        "\n",
        "comparison_results[\"Static Weighted\"] = {\n",
        "    \"Execution Time (s)\": exec_time,\n",
        "    \"Average CPU Usage (%)\": avg_cpu,\n",
        "    \"Peak RAM Usage (MB)\": peak_ram,\n",
        "    \"Total Inference Time (s)\": total_inference_time,\n",
        "    \"Average Inference Time per Sample (s)\": average_inference_time,\n",
        "    \"MAE\": mae_ensemble,\n",
        "    \"MSE\": mse_ensemble,\n",
        "    \"R² Score\": r2_ensemble,\n",
        "    \"Memory Usage (MB)\": memory_usage\n",
        "}\n",
        "\n",
        "# --- Stacking Ensemble ---\n",
        "exec_time, avg_cpu, peak_ram, memory_usage, y_pred_stack = measure_execution_time(\n",
        "    lambda: stacking_reg.predict(X_test)\n",
        ")\n",
        "\n",
        "# Measure inference time\n",
        "inf_time_start = time.time()\n",
        "y_pred_stack = stacking_reg.predict(X_test)\n",
        "inf_time_end = time.time()\n",
        "total_inference_time = inf_time_end - inf_time_start\n",
        "average_inference_time = total_inference_time / len(y_test)\n",
        "\n",
        "# Evaluate Performance\n",
        "mae_stacked = mean_absolute_error(y_test, y_pred_stack)\n",
        "mse_stacked = mean_squared_error(y_test, y_pred_stack)\n",
        "r2_stacked = r2_score(y_test, y_pred_stack)\n",
        "\n",
        "comparison_results[\"Stacking Ensemble\"] = {\n",
        "    \"Execution Time (s)\": exec_time,\n",
        "    \"Average CPU Usage (%)\": avg_cpu,\n",
        "    \"Peak RAM Usage (MB)\": peak_ram,\n",
        "    \"Total Inference Time (s)\": total_inference_time,\n",
        "    \"Average Inference Time per Sample (s)\": average_inference_time,\n",
        "    \"MAE\": mae_stacked,\n",
        "    \"MSE\": mse_stacked,\n",
        "    \"R² Score\": r2_stacked,\n",
        "    \"Memory Usage (MB)\": memory_usage\n",
        "}\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "comparison_results_df = pd.DataFrame.from_dict(comparison_results, orient=\"index\")\n",
        "\n",
        "# Display results\n",
        "display(comparison_results_df)\n"
      ],
      "metadata": {
        "id": "A3sr89Y4gLtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM8v4YWsP56b"
      },
      "source": [
        "# Step 10: Save & Deploy\n",
        "\n",
        "* This is if we decide Stack Ensembling Model is the best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYeDaNf9X1kz"
      },
      "source": [
        "## Step 10.1: Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur_W0fzMWzGS"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(stacking_reg, 'stacking_regressor.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9y70P1jX3Ce"
      },
      "source": [
        "## Step 10.2: Load the Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bO12mWJEX4-_"
      },
      "outputs": [],
      "source": [
        "# Load the trained model\n",
        "stacking_reg = joblib.load('stacking_regressor.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dLHRuRWYCKC"
      },
      "source": [
        "## Step 10.3: Prepare New Data for Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTN8evLIYIse"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "preset = { \"Age\": [30],\n",
        "        \"YrsExp\": [5],\n",
        "        \"EduLevel\": [\"BachelorDegree\"],\n",
        "        \"Gender\": [\"Male\"],\n",
        "        \"Industry\": [\"manager-product-operations\"],\n",
        "        \"Generation\": [\"Millennials\"],\n",
        "        \"Seniority\": [\"Junior\"]\n",
        "    }\n",
        "\n",
        "industry_titles = {\n",
        "    1: 'manager-product-operations',\n",
        "    2: 'developer-end-back',\n",
        "    3: 'senior-engineer-sales',\n",
        "    4: 'analyst-data-financial',\n",
        "    5: 'engineer-full-stack',\n",
        "    6: 'director-of-marketing',\n",
        "    7: 'marketing-manager-senior',\n",
        "    8: 'scientist-data-senior',\n",
        "    9: 'software-engineer-manager'\n",
        "}\n",
        "\n",
        "def get_industry_title(number):\n",
        "    # If the key (number) exists in the dictionary\n",
        "    if number in industry_titles:\n",
        "        return industry_titles[number]\n",
        "    else:\n",
        "        return \"Job title not found for this number.\"\n",
        "\n",
        "\n",
        "def derive_generation(age):\n",
        "    \"\"\"Automatically derive Generation based on Age\"\"\"\n",
        "    birth_year = 2023 - age\n",
        "    if birth_year <= 1943:\n",
        "        return \"SilentGeneration\"\n",
        "    elif birth_year <= 1964:\n",
        "        return \"BabyBoomer\"\n",
        "    elif birth_year <= 1979:\n",
        "        return \"GenX\"\n",
        "    elif birth_year <= 1994:\n",
        "        return \"Millennials\"\n",
        "    else:\n",
        "        return \"GenZ\"\n",
        "\n",
        "def derive_seniority(yrs_exp):\n",
        "    \"\"\"Automatically derive Seniority based on Years of Experience\"\"\"\n",
        "    if yrs_exp <= 3:\n",
        "        return \"Entry\"\n",
        "    elif 4 <= yrs_exp <= 6:\n",
        "        return \"Junior\"\n",
        "    elif 7 <= yrs_exp <= 10:\n",
        "        return \"Mid\"\n",
        "    elif 11 <= yrs_exp <= 15:\n",
        "        return \"Senior\"\n",
        "    else:\n",
        "        return \"Executive\"\n",
        "\n",
        "# Ask the user how they want to proceed\n",
        "print(\"\\nChoose Data Input Method:\")\n",
        "print(\"1 - Manually Enter Your Own Data\")\n",
        "print(\"2 - Load Pre-Prepared Data from Code\")\n",
        "\n",
        "choice = input(\"Enter 1 or 2: \").strip()\n",
        "\n",
        "if choice == \"1\":\n",
        "    # Manual User Input\n",
        "    age = int(input(\"Enter Age: \"))\n",
        "    yrs_exp = int(input(\"Enter Years of Experience: \"))\n",
        "    edu_level = input(\"Enter Education Level (HighSchool, BachelorDegree, MasterDegree, PhD): \")\n",
        "    gender = input(\"Enter Gender (Male/Female): \")\n",
        "    display(industry_titles)\n",
        "    industry_input = int(input(\"Enter a number of your closest industry (1-9): \"))\n",
        "    industry =  get_industry_title(industry_input)\n",
        "\n",
        "    # Auto-derive 'Generation' and 'Seniority'\n",
        "    generation = derive_generation(age)\n",
        "    seniority = derive_seniority(yrs_exp)\n",
        "\n",
        "    user_data = {\n",
        "        \"Age\": [age],\n",
        "        \"YrsExp\": [yrs_exp],\n",
        "        \"EduLevel\": [edu_level],\n",
        "        \"Gender\": [gender],\n",
        "        \"Industry\": [industry],\n",
        "        \"Generation\": [generation],\n",
        "        \"Seniority\": [seniority]\n",
        "    }\n",
        "\n",
        "    print(\"\\n User Data Entered Successfully!\\n\")\n",
        "\n",
        "elif choice == \"2\":\n",
        "    # Load Pre-Prepared Data\n",
        "    user_data = preset\n",
        "\n",
        "    print(\"\\n Loaded Pre-Prepared Data from Code!\\n\")\n",
        "\n",
        "else:\n",
        "    print(\"Invalid choice. Defaulting to pre-prepared data.\")\n",
        "    user_data = preset\n",
        "\n",
        "# Convert user input to DataFrame\n",
        "df_new = pd.DataFrame(user_data)\n",
        "\n",
        "# Display the selected data\n",
        "print(\"\\n Final Input Data:\")\n",
        "print(df_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLfVaMLUYKjw"
      },
      "source": [
        "Preprocess New Data (Apply Same Encoding and Scaling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YSQktoKYN46"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
        "\n",
        "# Load encoders used during training\n",
        "le_education = LabelEncoder()\n",
        "le_education.classes_ = np.array(['HighSchool', 'BachelorDegree', 'MasterDegree', 'PhD'])\n",
        "df_new['EduLevel'] = le_education.transform(df_new['EduLevel'])\n",
        "\n",
        "le_generation = LabelEncoder()\n",
        "le_generation.classes_ = np.array(['SilentGeneration', 'BabyBoomer', 'GenX', 'Millennials', 'GenZ'])\n",
        "df_new['Generation'] = le_generation.transform(df_new['Generation'])\n",
        "\n",
        "le_seniority = LabelEncoder()\n",
        "le_seniority.classes_ = np.array(['Entry', 'Junior', 'Mid', 'Senior', 'Executive'])\n",
        "df_new['Seniority'] = le_seniority.transform(df_new['Seniority'])\n",
        "\n",
        "# OneHotEncode Gender\n",
        "ohe_gender = OneHotEncoder(categories=[['Female', 'Male']], drop='first', sparse_output=False)\n",
        "gender_encoded = ohe_gender.fit_transform(df_new[['Gender']])  # Apply same encoding as in training\n",
        "gender_columns = ohe_gender.get_feature_names_out(['Gender'])\n",
        "df_gender_encoded = pd.DataFrame(gender_encoded, columns=gender_columns)\n",
        "\n",
        "# OneHotEncode Industry\n",
        "ohe_industry = OneHotEncoder(categories=[[\n",
        "    'manager-product-operations', 'developer-end-back',\n",
        "    'senior-engineer-sales', 'analyst-data-financial',\n",
        "    'engineer-full-stack', 'director-of-marketing',\n",
        "    'marketing-manager-senior', 'scientist-data-senior',\n",
        "    'software-engineer-manager'\n",
        "]], drop='first', sparse_output=False)\n",
        "\n",
        "industry_encoded = ohe_industry.fit_transform(df_new[['Industry']])  # Ensure same encoding\n",
        "industry_columns = ohe_industry.get_feature_names_out(['Industry'])\n",
        "df_industry_encoded = pd.DataFrame(industry_encoded, columns=industry_columns)\n",
        "\n",
        "# **Ensure all expected columns are present (to match training features)**\n",
        "df_gender_encoded = df_gender_encoded.reindex(columns=['Gender_Male'], fill_value=0)  # Only 'Male' exists due to drop='first'\n",
        "df_industry_encoded = df_industry_encoded.reindex(columns=industry_columns, fill_value=0)\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "df_new[['Age', 'YrsExp']] = scaler.fit_transform(df_new[['Age', 'YrsExp']])\n",
        "\n",
        "# **Combine processed data**\n",
        "df_processed = pd.concat([\n",
        "    df_new[['Age', 'YrsExp', 'EduLevel', 'Generation', 'Seniority']],\n",
        "    df_gender_encoded, df_industry_encoded\n",
        "], axis=1)\n",
        "\n",
        "# **Ensure final feature order matches training data**\n",
        "df_processed = df_processed.reindex(columns=X_final.columns, fill_value=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJw7qtYetSfO"
      },
      "source": [
        "Test if any missing features between user and saved model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gai08Mw0mBR9"
      },
      "outputs": [],
      "source": [
        "# Get expected feature names from training data\n",
        "expected_features = X_final.columns.tolist()\n",
        "\n",
        "# Get actual feature names from the new input\n",
        "actual_features = df_processed.columns.tolist()\n",
        "\n",
        "# Find the missing feature\n",
        "missing_features = list(set(expected_features) - set(actual_features))\n",
        "extra_features = list(set(actual_features) - set(expected_features))\n",
        "\n",
        "print(f\"Missing feature(s): {missing_features}\")\n",
        "print(f\"Extra feature(s): {extra_features}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd_woAqOYRbb"
      },
      "source": [
        "## Step 10.4: Make Predictions\n",
        "\n",
        "*  Once preprocessing is complete, pass the transformed data to your trained model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UcAga_CYQqH"
      },
      "outputs": [],
      "source": [
        "predicted_salary = stacking_reg.predict(df_processed.to_numpy())\n",
        "\n",
        "# Calculate absolute error between predicted salary and actual test values\n",
        "errors = np.abs(y_test - predicted_salary[0])\n",
        "\n",
        "# Find the closest actual salary\n",
        "closest_index = np.argmin(errors)\n",
        "closest_actual_salary = y_test.iloc[closest_index]\n",
        "\n",
        "# Calculate percentage error\n",
        "percentage_error = (abs(predicted_salary[0] - closest_actual_salary) / closest_actual_salary) * 100\n",
        "\n",
        "# Print results\n",
        "print(f\"Predicted Salary: {predicted_salary[0]:,.2f}\")\n",
        "print(f\"Closest Actual Salary: {closest_actual_salary:,.2f}\")\n",
        "print(f\"Percentage Error: {percentage_error:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10.5: Visualisation"
      ],
      "metadata": {
        "id": "DKckzW_Tsw-7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrjHjy11sr2I"
      },
      "source": [
        "ScatterPlot to Demonstrate Accuracy\n",
        "\n",
        "*   Blue dots → Predictions from the test set\n",
        "*   Red dashed line → Ideal prediction line (y = x)\n",
        "*   Red 'X' marker → The newly predicted salary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCBJwNlfrZtE"
      },
      "outputs": [],
      "source": [
        "# Predict salaries for test set\n",
        "y_pred_test = stacking_reg.predict(X_test)\n",
        "\n",
        "# Create the scatter plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, y_pred_test, alpha=0.4, label=\"Model Predictions\")\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label=\"Perfect Prediction (y = x)\")\n",
        "\n",
        "# Mark the new predicted salary\n",
        "plt.scatter(predicted_salary[0], predicted_salary[0], color='red', s=150, marker='x', label=\"New Prediction\")\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel(\"Actual Salary\")\n",
        "plt.ylabel(\"Predicted Salary\")\n",
        "plt.title(\"Predicted vs. Actual Salary\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll-AV-pisoET"
      },
      "source": [
        "2D histogram (hexbin plot) to show density:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AX_7Cs4wseGn"
      },
      "outputs": [],
      "source": [
        "plt.hexbin(y_test, y_pred_test, gridsize=50, cmap='Blues', alpha=0.8)\n",
        "plt.colorbar(label='Density of Points')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MAQb5CduErS"
      },
      "outputs": [],
      "source": [
        "# Plot error distribution\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.hist(errors, bins=30, color='skyblue', alpha=0.7)\n",
        "plt.axvline(x=np.abs(predicted_salary[0] - closest_actual_salary), color='red', linestyle='dashed', linewidth=2, label=\"New Prediction Error\")\n",
        "plt.xlabel(\"Absolute Error\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Prediction Errors\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ]
}