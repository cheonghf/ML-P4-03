{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZNM7PcHfGSyD64wz24YF5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cheonghf/ML-P4-03/blob/main/Project_SourceCode_P4_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr87tkF0df2f"
      },
      "source": [
        "Import Salary Data CSV from Kaggle to Colab as Dataframe\n",
        "\n",
        "*   Dataset is sourced from https://www.kaggle.com/datasets/mohithsairamreddy/salary-data/data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffo7mgqpf0NV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Download latest version\n",
        "dataset_ref = kagglehub.dataset_download('mohithsairamreddy/salary-data', path='Salary_Data.csv')\n",
        "\n",
        "#dataframe salary data 1\n",
        "df_sd1 = pd.read_csv(dataset_ref)\n",
        "\n",
        "#copy dataframe salary data 1 to dataframe salary data 2\n",
        "df_sd2 = df_sd1.copy()\n",
        "\n",
        "print('Original Data Frame:', df_sd1.shape)\n",
        "print('Copied Data Frame:', df_sd2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWtPtZh5O0g4"
      },
      "source": [
        "#Step 1: Pre-Processing of Data\n",
        "\n",
        "*   Run Everything\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-bUx4Co2O4z"
      },
      "source": [
        "View Columns of 'Salary Data' Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBVDEYAg2T5G"
      },
      "outputs": [],
      "source": [
        "print(df_sd2.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uorwj6Qt2aR2"
      },
      "source": [
        "Rename Columns in 'Salary Data' Dataframe by simplyfing and removing spaces, this allows us to better track for ease of programming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRnAan2l2dex"
      },
      "outputs": [],
      "source": [
        "df_sd2.rename(columns={\"Education Level\": \"EduLevel\", \"Job Title\": \"JobTitle\", \"Years of Experience\": \"YrsExp\"}, inplace=True)\n",
        "print(\"Updated Columns are:\", df_sd2.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1.1: What did we do if our Dataframe has Null values?"
      ],
      "metadata": {
        "id": "6ZI_76zcEUYx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIa-uJFgdnsv"
      },
      "source": [
        "View the full 'Salary Data' Dataframe and Identify if it holds any Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plbhaO5agG5o"
      },
      "outputs": [],
      "source": [
        "# view dataset salary data 2\n",
        "print(df_sd2)\n",
        "print(\"===Data Frame End===\")\n",
        "print('')\n",
        "\n",
        "# Identify columns with missing valuesin dataset salary data 2\n",
        "print('Null Values are at:')\n",
        "print(df_sd2.isnull().sum())\n",
        "print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No2GkgGBeg83"
      },
      "source": [
        "1. Remove identified Null Values from the Dataframe\n",
        "2. Run a check on the DataFrame to confirm if the Null Values are removed\n",
        "3. We will go a step further and check if hidden Null Values could be represented by a string 'Na'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3enNgXyen-2"
      },
      "outputs": [],
      "source": [
        "# Remove rows with missing values\n",
        "df_sd2.dropna(inplace=True)\n",
        "\n",
        "# Check if columns with missing values are removed\n",
        "print(df_sd2.isnull().sum())\n",
        "print('')\n",
        "\n",
        "# To check if 'na' is string instead of just null\n",
        "for column in df_sd2.columns:\n",
        "    na_count = df_sd2[column].astype(str).apply(lambda x: x == 'na').sum()\n",
        "    if na_count > 0:\n",
        "        print(f\"Column '{column}' contains {na_count} rows with the string 'na'\")\n",
        "    else:\n",
        "      print(f\"Column '{column}' does not contain the string 'na'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1.2: What did we do if our Dataframe has Duplicate Entries?"
      ],
      "metadata": {
        "id": "_LKEVWO1Eo1j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VNAsH8XYNHh"
      },
      "source": [
        "Check for Duplicate Entries in 'Salary Data' Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7EBl8oqXfz9"
      },
      "outputs": [],
      "source": [
        "# Check for duplicates and create a boolean Series\n",
        "duplicates = df_sd2.duplicated()\n",
        "\n",
        "# Count the number of duplicate rows\n",
        "num_duplicates = duplicates.sum()\n",
        "\n",
        "# Calculate the number of non-duplicate rows\n",
        "num_non_duplicates = len(df_sd2) - num_duplicates\n",
        "\n",
        "# Calculate the percentage of duplicates\n",
        "percentage_duplicates = (num_duplicates / len(df_sd2)) * 100\n",
        "\n",
        "# Print the results\n",
        "print(f\"Number of duplicate rows: {num_duplicates}\")\n",
        "print(f\"Number of non-duplicate rows: {num_non_duplicates}\")\n",
        "print(f\"Percentage of duplicate rows: {percentage_duplicates:.2f}%\")\n",
        "print('')\n",
        "\n",
        "# View the actual duplicate rows:\n",
        "if num_duplicates > 0:\n",
        "  print(\"Duplicate rows:\")\n",
        "  print(df_sd2[duplicates])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM4_K6_Cb0XJ"
      },
      "source": [
        "Filter Duplicate Rows of Data into another Data Frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlxPmQfVbz9w"
      },
      "outputs": [],
      "source": [
        "# Checking duplicates distribution\n",
        "duplicate_counts = df_sd2[df_sd2.duplicated()].groupby(df_sd2.columns.tolist()).size().reset_index(name=\"Count\")\n",
        "\n",
        "# Identifying potential duplication patterns\n",
        "df_duplicates = df_sd2.groupby(df_sd2.columns.tolist()).size().reset_index(name='Duplicate Count')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxgbJ0w-cZNH"
      },
      "source": [
        "Histogram Comparison for 'Age' Duplicate Filter Before Vs After"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GF0xEox3cJIT"
      },
      "outputs": [],
      "source": [
        "# Visualising Age distribution before and after removing duplicates\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Before removing duplicates\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_sd2['Age'].dropna(), bins=20, kde=True, color='blue')\n",
        "plt.title(\"Age Distribution Before Removing Duplicates\")\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "# After removing duplicates\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(df_duplicates['Age'].dropna(), bins=20, kde=True, color='green')\n",
        "plt.title(\"Age Distribution After Removing Duplicates\")\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZQc154obbd0"
      },
      "source": [
        "Histogram Comparison for 'Salary' Duplicate Filter Before Vs After"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYbfXkERZ51h"
      },
      "outputs": [],
      "source": [
        "# Visualising salary distribution before and after removing duplicates\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Before removing duplicates\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_sd2['Salary'].dropna(), bins=20, kde=True, color='blue')\n",
        "plt.title(\"Salary Distribution Before Removing Duplicates\")\n",
        "plt.xlabel(\"Salary\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "# After removing duplicates\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(df_duplicates['Salary'].dropna(), bins=20, kde=True, color='green')\n",
        "plt.title(\"Salary Distribution After Removing Duplicates\")\n",
        "plt.xlabel(\"Salary\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elrt9zFKkej8"
      },
      "source": [
        "Histogram Comparison for 'YrsExp' Duplicate Filter Before Vs After"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsl-qqb1cj7-"
      },
      "outputs": [],
      "source": [
        "# Visualising Years of Experience distribution before and after removing duplicates\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Before removing duplicates\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_sd2['YrsExp'].dropna(), bins=20, kde=True, color='blue')\n",
        "plt.title(\"Years of Experience Distribution Before Removing Duplicates\")\n",
        "plt.xlabel(\"Years of Experience\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "# After removing duplicates\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(df_duplicates['YrsExp'].dropna(), bins=20, kde=True, color='green')\n",
        "plt.title(\"Years of Experience Distribution After Removing Duplicates\")\n",
        "plt.xlabel(\"Years of Experience\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba57JNxgdsVV"
      },
      "source": [
        "Removing Duplicates from the Dataframe\n",
        "\n",
        "*   We chose to remove them to prevent our model from overfitting\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPqqrS87dvpX"
      },
      "outputs": [],
      "source": [
        "# Remove duplicate rows and keep the first occurrence\n",
        "df_sd3 = df_sd2.drop_duplicates(keep='first')\n",
        "\n",
        "# Reset the index\n",
        "df_sd3 = df_sd3.reset_index(drop=True)\n",
        "\n",
        "# Print the shape of the DataFrame before and after removing duplicates\n",
        "print(\"Original DataFrame shape:\", df_sd2.shape)\n",
        "print(\"DataFrame shape after removing duplicates:\", df_sd3.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1.3: Identifying if there are any neglible data to remove from the 'Salary Data' Dataframe"
      ],
      "metadata": {
        "id": "Nj6pWLjbFCdI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSpm8HnskvEj"
      },
      "source": [
        "Understanding values inside 'Salary Data' Dataframe\n",
        "\n",
        "* Data Type in each columns\n",
        "* Summary Statistic for each numerical columns\n",
        "* Frequency count for unique data in each columns\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KJj2Mh1ex7Q"
      },
      "outputs": [],
      "source": [
        "# view dataset salary data 3 information\n",
        "print(df_sd3.info())\n",
        "print('')\n",
        "\n",
        "# Statistic range of dataset salary data 3\n",
        "print(df_sd3.describe())\n",
        "print('')\n",
        "\n",
        "string_columns = df_sd3.select_dtypes(include=['object']).columns\n",
        "\n",
        "for column in string_columns:\n",
        "    frequency_table = df_sd3[column].value_counts().to_frame()\n",
        "    print(f\"Column: {column}\")\n",
        "    print(frequency_table)  # Optionally add .to_string() for better formatting\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VccH4825TlcZ"
      },
      "source": [
        "To determine if 'Other' in 'Gender' column is significant ( less than 2%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-trmUDVwP8jy"
      },
      "outputs": [],
      "source": [
        "gender_counts = df_sd3['Gender'].value_counts()\n",
        "other_percentage = (gender_counts['Other'] / len(df_sd3)) * 100\n",
        "print(f\"Percentage of 'Other' in Gender: {other_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PakStBNOKnv"
      },
      "source": [
        "Remove 'Other' and its relevant row of data from the column 'Gender'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcBnmeV9OM-h"
      },
      "outputs": [],
      "source": [
        "# Filter out rows where Gender is 'Other'\n",
        "df_sd3 = df_sd3[df_sd3['Gender'] != 'Other']\n",
        "\n",
        "# Reset the index if needed\n",
        "df_sd3 = df_sd3.reset_index(drop=True)\n",
        "\n",
        "# Display unique values after cleaning\n",
        "print('Updated Unique Values under \"Gender\":', df_sd3['Gender'].unique())\n",
        "\n",
        "# Display shape\n",
        "print(\"DataFrame shape after cleaning Gender:\", df_sd3.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi-GCmbZRfwq"
      },
      "source": [
        "Standardise naming of Data under the column 'EduLevel'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtrzRi6vRe8k"
      },
      "outputs": [],
      "source": [
        "# Define a mapping dictionary for standardisation\n",
        "education_mapping = {\n",
        "    \"Bachelor's Degree\": \"BachelorDegree\",\n",
        "    \"Bachelor's\": \"BachelorDegree\",\n",
        "    \"Master's Degree\": \"MasterDegree\",\n",
        "    \"Master's\": \"MasterDegree\",\n",
        "    \"phD\": \"PhD\",\n",
        "    \"High School\": \"HighSchool\"\n",
        "}\n",
        "\n",
        "# Apply mapping to clean the 'EduLevel' column\n",
        "df_sd3['EduLevel'] = df_sd3['EduLevel'].replace(education_mapping)\n",
        "\n",
        "# Display unique values after cleaning\n",
        "print('Updated Unique Values in Data Frame:', df_sd3['EduLevel'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MUIgMK0OiC8"
      },
      "source": [
        "# Step 2: Analsying the DataSet (Run Step 1 Before this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I57xItm91LrF"
      },
      "source": [
        "Grouped Bar Chart: Gender Breakdown by Education Level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQc-4sr4ptJN"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))  # Adjust figure size if needed\n",
        "sns.countplot(x='EduLevel', hue='Gender', data=df_sd3)\n",
        "plt.title('Breakdown of Gender by Education Level')\n",
        "plt.xlabel('Education Level')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK-AqpBnPdNy"
      },
      "source": [
        "Histogram: Age Distribution of Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QSiLeROOKCj"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))  # Adjust figure size if needed\n",
        "sns.histplot(df_sd3['Age'], bins=20, kde=True)  # 'bins' controls the number of bars, 'kde' adds a density curve\n",
        "plt.title('Age Distribution')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tSsrUJS9rC7"
      },
      "source": [
        "BoxPlot: Salary Distribution by Gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUJ4MtiL9j-9"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))  # Adjust figure size if needed\n",
        "sns.boxplot(x='Gender', y='Salary', data=df_sd3)\n",
        "plt.title('Salary Distribution by Gender')\n",
        "plt.xlabel('Gender')\n",
        "plt.ylabel('Salary')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXIvlyQ11XFs"
      },
      "source": [
        "Box Plot Salary Distribution by Education Level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FZghjFc1TCJ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='EduLevel', y='Salary', data=df_sd3)\n",
        "plt.title('Salary Distribution by Education Level')\n",
        "plt.xlabel('Education Level')\n",
        "plt.ylabel('Salary')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsN7n9nD2Aw9"
      },
      "source": [
        "Scatterplot of Salary Distribution by Years of Experience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQt2f2Ei11gp"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.scatterplot(x='YrsExp', y='Salary', data=df_sd3)\n",
        "plt.title('Salary Distribution by Years of Experience')\n",
        "plt.xlabel('Years of Experience')\n",
        "plt.ylabel('Salary')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f1LF4UElM7m"
      },
      "source": [
        "Scatterplot of Salary Distribution by Age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB5gF22Z2kOd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.scatterplot(x='Age', y='Salary', data=df_sd3)\n",
        "plt.title('Salary Distribution by Age')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Salary')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heatmap for numerical feature relationships of Dataset"
      ],
      "metadata": {
        "id": "R_wtpsIwB6qW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show correlation heat map and matrix\n",
        "df_corr = df_sd3[['Age','YrsExp','Salary']]\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "corrmat = df_corr.corr()\n",
        "\n",
        "# Title\n",
        "plt.title(\"Numerical Correlation Heatmap\")\n",
        "\n",
        "# Generate the heatmap\n",
        "sns.heatmap(corrmat, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zL37CbvIBX-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heatmap for categorical features relationships of Dataset"
      ],
      "metadata": {
        "id": "MXxIg47tBZHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cramers_v(confusion_matrix):\n",
        "    \"\"\"Calculate Cramér's V statistic for association between two categorical variables\"\"\"\n",
        "    chi2 = stats.chi2_contingency(confusion_matrix)[0]  # Get Chi-square value\n",
        "    n = confusion_matrix.sum().sum()  # Total observations\n",
        "    k = min(confusion_matrix.shape)  # Minimum of row/column count\n",
        "    return np.sqrt(chi2 / (n * (k - 1)))\n",
        "\n",
        "# List of categorical variables\n",
        "categorical_vars = ['Gender', 'EduLevel', 'JobTitle']\n",
        "\n",
        "# Compute Cramér's V matrix\n",
        "cramers_v_matrix = pd.DataFrame(np.zeros((len(categorical_vars), len(categorical_vars))),\n",
        "                                index=categorical_vars, columns=categorical_vars)\n",
        "\n",
        "for i, var1 in enumerate(categorical_vars):\n",
        "    for j, var2 in enumerate(categorical_vars):\n",
        "        if i == j:\n",
        "            cramers_v_matrix.iloc[i, j] = 1.0  # Diagonal should be 1 (same variable)\n",
        "        else:\n",
        "            contingency_table = pd.crosstab(df_sd3[var1], df_sd3[var2])\n",
        "            cramers_v_matrix.iloc[i, j] = cramers_v(contingency_table)\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cramers_v_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Cramér's V Heatmap for Categorical Variables\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5bS_0kqQ_0ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Machine Learning Algorithm Implementation with Raw Data (Run Step 1 Before this)\n",
        "\n",
        "1. We will encode the relevant columns of data that fit our problem statement\n",
        "2. Next, we will spilt the Dataset\n",
        "3. Lastly, we will run 4 different types of Machine Learning Algorithms to identify best two\n"
      ],
      "metadata": {
        "id": "pldeuQ8wU38p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l6GPRn2803i"
      },
      "source": [
        "Copy Dataframe for Step 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOfitDdJ8udm"
      },
      "outputs": [],
      "source": [
        "df_sd4 = df_sd3.copy()\n",
        "\n",
        "print('Original Data Frame:', df_sd3.shape)\n",
        "print('Copied Data Frame:', df_sd4.shape)\n",
        "print()\n",
        "df_sd4.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding raw data from pre-processed 'Salary Data' Dataframe"
      ],
      "metadata": {
        "id": "MjvnD9tkWUwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
        "\n",
        "# Drop 'JobTitle' (high cardinality, too specific)\n",
        "df_sd5 = df_sd4.drop(columns=['JobTitle'])\n",
        "\n",
        "# Separate features and target\n",
        "X = df_sd5.drop(columns=['Salary'])\n",
        "y = df_sd5['Salary']\n",
        "\n",
        "# Identify feature types\n",
        "categorical_ordinal = ['EduLevel']  # Ordinal categories\n",
        "categorical_nominal = ['Gender']  # Nominal categories\n",
        "numerical_cols = ['Age', 'YrsExp']\n",
        "\n",
        "# Apply Label Encoding to ordinal categorical variables\n",
        "for col in categorical_ordinal:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Apply OneHotEncoding to nominal categorical variables\n",
        "ohe = OneHotEncoder(sparse_output=False)  # Avoid dummy variable trap\n",
        "X_encoded = ohe.fit_transform(X[categorical_nominal])\n",
        "encoded_feature_names = ohe.get_feature_names_out(categorical_nominal)\n",
        "\n",
        "# Scale numerical variables\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X[numerical_cols])\n",
        "\n",
        "# Combine all transformed features\n",
        "X_processed = np.hstack((X_scaled, X[categorical_ordinal].values, X_encoded))\n",
        "\n",
        "# Convert to DataFrame for clarity\n",
        "X_final = pd.DataFrame(X_processed, columns=numerical_cols + categorical_ordinal + list(encoded_feature_names))\n",
        "\n",
        "X_final.columns"
      ],
      "metadata": {
        "id": "EN2n-hnwWhgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spilting of Training Data (Must Run before Algorithm)"
      ],
      "metadata": {
        "id": "vVfnmQQc4cd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Train-test split (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "1947CumL4bB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3.1: Among the four, we will pick the best two performing Machine Learning Algorithms to improve on it"
      ],
      "metadata": {
        "id": "043PF1TbG6je"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNAr7a204xEn"
      },
      "source": [
        "**Algorithm 1**: LinearRegression (Train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5z1mN_6w4xEo"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Train Linear Regression model\n",
        "lr_model = LinearRegression() # Initialise\n",
        "lr_model.fit(X_train, y_train) # Train the model on training data\n",
        "\n",
        "# Predict salaries on the test set\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "\n",
        "# Calculate error metrics\n",
        "mae_lr = mean_absolute_error(y_test, y_pred_lr)   # Mean Absolute Error\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)    # Mean Squared Error\n",
        "r2_lr = r2_score(y_test, y_pred_lr)               # R-squared score\n",
        "\n",
        "# Print performance metrics\n",
        "print(\"Linear Regression Performance:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_lr:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_lr:.2f}\")\n",
        "print(f\"R² Score: {r2_lr:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oQVJtU54xEp"
      },
      "source": [
        "**Algorithm 2**: DecisionTreeRegressor (Train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMqYR-lP4xEq"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "dtr_model = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "dtr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict salaries on the test set\n",
        "y_pred_dtr = dtr_model.predict(X_test)\n",
        "\n",
        "# Calculate error metrics\n",
        "mae_dtr = mean_absolute_error(y_test, y_pred_dtr)   # Mean Absolute Error\n",
        "mse_dtr = mean_squared_error(y_test, y_pred_dtr)    # Mean Squared Error\n",
        "r2_dtr = r2_score(y_test, y_pred_dtr)               # R-squared score\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"Decision Tree Performance:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_dtr:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_dtr:.2f}\")\n",
        "print(f\"R² Score: {r2_dtr:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkNkMAYJ4xEq"
      },
      "source": [
        "**Algorithm 3**: RandomForestRegressor (Train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-waDSVk-4xEr"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "# rfr_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rfr_model = RandomForestRegressor(random_state=42)\n",
        "rfr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict salaries on the test set\n",
        "y_pred_rfr = rfr_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "mae_rfr = mean_absolute_error(y_test,y_pred_rfr)\n",
        "mse_rfr = mean_squared_error(y_test, y_pred_rfr)    # Mean Squared Error\n",
        "r2_rfr = r2_score(y_test, y_pred_rfr)\n",
        "\n",
        "# Print results\n",
        "print(\"Random Forest Regressor Performance:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_rfr:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_rfr:.2f}\")\n",
        "print(f\"R² Score: {r2_rfr:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmCQWiTF4xEr"
      },
      "source": [
        "**Algorithm 4**: XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8LbE-H64xEr"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "xgb_model = XGBRegressor(n_estimators=200, learning_rate=0.05, random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict salaries on the test set\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost Performance\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_xgb:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_xgb:.2f}\")\n",
        "print(f\"R² Score: {r2_xgb:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall Table and Comparison Chart for ease of determining each Algorithm strengths"
      ],
      "metadata": {
        "id": "mMls_x86K4TM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data: Replace with actual values\n",
        "model_outputdata = {\n",
        "    \"Model\": [\"Linear Regression\", \"Decision Tree\", \"Random Forest\", \"XGBoost\"],\n",
        "    \"MAE\": [mae_lr, mae_dtr, mae_rfr, mae_xgb],\n",
        "    \"MSE\": [mse_lr, mse_dtr, mse_rfr, mse_xgb],\n",
        "    \"R² Score\": [r2_lr, r2_dtr, r2_rfr, r2_xgb]\n",
        "}\n",
        "\n",
        "df_performance = pd.DataFrame(model_outputdata)\n",
        "\n",
        "# Display table\n",
        "print(df_performance)\n",
        "\n",
        "# Create figure and axis\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "x = np.arange(len(df_performance[\"Model\"]))  # X locations\n",
        "width = 0.3  # Bar width\n",
        "\n",
        "# Plot MAE & MSE as bars\n",
        "ax1.bar(x - width/2, df_performance[\"MAE\"], width, label=\"MAE\", color=\"blue\", alpha=0.7)\n",
        "ax1.bar(x + width/2, df_performance[\"MSE\"], width, label=\"MSE\", color=\"orange\", alpha=0.7)\n",
        "\n",
        "# Add MAE Trend Line\n",
        "ax1.plot(x, df_performance[\"MAE\"], marker=\"o\", linestyle=\"-\", color=\"red\", linewidth=2, label=\"MAE Trend\")\n",
        "\n",
        "# Labels & Formatting\n",
        "ax1.set_xlabel(\"Models\")\n",
        "ax1.set_ylabel(\"Error Values (MAE & MSE)\")\n",
        "ax1.set_title(\"Model Performance Comparison with MAE Trend\")\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(df_performance[\"Model\"], rotation=15)\n",
        "ax1.set_yscale(\"log\")  # Log scale for better readability\n",
        "ax1.legend(loc=\"upper left\")\n",
        "\n",
        "# Create second Y-axis for R² Score\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(x, df_performance[\"R² Score\"], marker=\"s\", linestyle=\"-\", color=\"green\", linewidth=2, label=\"R² Score\")\n",
        "ax2.set_ylabel(\"R² Score\")\n",
        "ax2.set_ylim(0, 1)  # R² Score is always between 0 and 1\n",
        "ax2.legend(loc=\"upper right\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zzAeymELIEgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j3fQpTRj5B0"
      },
      "source": [
        "#Step 4: Additional Features Extraction (Run Step 1 Before this)\n",
        "\n",
        "*   Run Everything\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy Dataframe for Step 4"
      ],
      "metadata": {
        "id": "voce0BPzsvXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sd6 = df_sd3.copy()\n",
        "\n",
        "print('Original Data Frame:', df_sd3.shape)\n",
        "print('Copied Data Frame:', df_sd6.shape)\n",
        "print()\n",
        "\n",
        "print(df_sd6['JobTitle'].value_counts())"
      ],
      "metadata": {
        "id": "pUeDoVqm6ow_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4.1: Additional Feature 1 ('Generation'), Clustering 'Age'"
      ],
      "metadata": {
        "id": "I-I5hpOUxiz7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URV0eIoKrVW-"
      },
      "outputs": [],
      "source": [
        "# Define the reference year based on the dataset's last update\n",
        "reference_year = 2023\n",
        "\n",
        "# Calculate the Birth Year\n",
        "df_sd6['BirthYear'] = reference_year - df_sd6['Age']\n",
        "\n",
        "# Define bins and labels for generation categories\n",
        "bins = [0, 1943, 1964, 1979, 1994, float('inf')]\n",
        "labels = ['SilentGeneration', 'BabyBoomer', 'GenX', 'Millennials', 'GenZ']\n",
        "\n",
        "# Assign generations based on Birth Year\n",
        "df_sd6['Generation'] = pd.cut(df_sd6['BirthYear'], bins=bins, labels=labels, right=True)\n",
        "\n",
        "# Get the frequency of each label under 'Generation'\n",
        "generation_counts = df_sd6['Generation'].value_counts()\n",
        "\n",
        "# Print the frequency\n",
        "print(\"Frequency of each generation type:\")\n",
        "print(generation_counts)\n",
        "print('')\n",
        "\n",
        "# Create a pie chart\n",
        "plt.figure(figsize=(8, 8))  # Adjust figure size if needed\n",
        "# Remove categories with zero count\n",
        "filtered_counts = generation_counts[generation_counts > 0]\n",
        "\n",
        "plt.pie(filtered_counts, labels=filtered_counts.index,\n",
        "        autopct='%1.1f%%', startangle=90, textprops={'fontsize': 12})\n",
        "plt.title('Distribution of Generations')\n",
        "plt.show()\n",
        "print('')\n",
        "\n",
        "print(df_sd6.shape)\n",
        "print(\"Updated Columns of Data Set\", df_sd6.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kpc9dcMkuqFo"
      },
      "source": [
        "## Step 4.2: Additional Feature 1 ('Seniority'), Clustering 'YrsExp'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWiZXhQwvDgq"
      },
      "outputs": [],
      "source": [
        "def assign_seniority(years):\n",
        "    if years <= 3:\n",
        "        return \"Entry\"\n",
        "    elif 4 <= years <= 6:\n",
        "        return \"Junior\"\n",
        "    elif 7 <= years <= 10:\n",
        "        return \"Mid\"\n",
        "    elif 11 <= years <= 15:\n",
        "        return \"Senior\"\n",
        "    else:\n",
        "        return \"Executive\"\n",
        "\n",
        "# Apply the mapping function to create the new 'Seniority' column\n",
        "df_sd6['Seniority'] = df_sd6['YrsExp'].apply(assign_seniority)\n",
        "\n",
        "# Get the frequency of each label under 'Generation'\n",
        "seniority_counts = df_sd6['Seniority'].value_counts()\n",
        "\n",
        "# Print the frequency\n",
        "print(\"Frequency of each Seniority:\")\n",
        "print(seniority_counts)\n",
        "print('')\n",
        "\n",
        "df_sd6.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4.3: Additional Feature 3 ('Industry'), Clustering of 'JobTitle'"
      ],
      "metadata": {
        "id": "ifDCetaYxY-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Imports relevant libaries\n",
        "* Cleans job titles (lowercase, removes special characters)\n",
        "* Tokenises job titles into words"
      ],
      "metadata": {
        "id": "yma2Qq_EwlTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, pairwise_distances\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "from collections import Counter\n",
        "\n",
        "def preprocess_job_title(title):\n",
        "    title = title.lower()  # Convert to lowercase\n",
        "    title = re.sub(r'[^a-z\\s]', '', title)  # Remove special characters\n",
        "    return title.split()  # Tokenize by splitting on spaces\n",
        "\n",
        "# Apply preprocessing to job titles\n",
        "df_sd6[\"Cleaned_Job_Title\"] = df_sd6[\"JobTitle\"].astype(str).apply(preprocess_job_title)"
      ],
      "metadata": {
        "id": "d-n92mi6ztZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Trains a Word2Vec model on job titles\n",
        "* Converts job titles into vector embeddings"
      ],
      "metadata": {
        "id": "-ab9YwxLwzbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Train Word2Vec for Job Title Embeddings\n",
        "word2vec_model = Word2Vec(sentences=df_sd6[\"Cleaned_Job_Title\"], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "def get_vector(title_tokens):\n",
        "    vectors = [word2vec_model.wv[word] for word in title_tokens if word in word2vec_model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(100)\n",
        "\n",
        "# Generate word embeddings\n",
        "df_sd6[\"Job_Title_Vector\"] = df_sd6[\"Cleaned_Job_Title\"].apply(get_vector)\n",
        "X = np.vstack(df_sd6[\"Job_Title_Vector\"].values)"
      ],
      "metadata": {
        "id": "ywyF2BfMz1a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Uses Elbow Method (inertia) and Silhouette Score to find the best number of clusters"
      ],
      "metadata": {
        "id": "6UqyVDL0xA2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Determine the Optimal Number of Clusters\n",
        "inertia = []\n",
        "silhouette_scores = []\n",
        "cluster_range = range(2, 11)\n",
        "\n",
        "for k in cluster_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    cluster_labels = kmeans.fit_predict(X)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "    silhouette_scores.append(silhouette_score(X, cluster_labels))\n",
        "\n",
        "# Plot Elbow & Silhouette Score Side-by-Side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "axes[0].plot(cluster_range, inertia, marker='o', linestyle='-', label='Inertia')\n",
        "axes[0].set_xlabel(\"Number of Clusters\")\n",
        "axes[0].set_ylabel(\"Inertia\")\n",
        "axes[0].set_title(\"Elbow Method for Optimal Clusters\")\n",
        "axes[0].legend()\n",
        "axes[0].grid()\n",
        "\n",
        "axes[1].plot(cluster_range, silhouette_scores, marker='o', linestyle='-', label='Silhouette Score', color='red')\n",
        "axes[1].set_xlabel(\"Number of Clusters\")\n",
        "axes[1].set_ylabel(\"Silhouette Score\")\n",
        "axes[1].set_title(\"Silhouette Score for Cluster Quality\")\n",
        "axes[1].legend()\n",
        "axes[1].grid()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_yLH8KZWz4Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Selects the best cluster count based on the highest Silhouette Score\n",
        "* Assigns job titles to clusters"
      ],
      "metadata": {
        "id": "Wz9TW5vJxE93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Apply K-Means Clustering Using Optimal Cluster Count\n",
        "optimal_clusters = cluster_range[silhouette_scores.index(max(silhouette_scores))]  # Best Silhouette Score\n",
        "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42, n_init=10)\n",
        "df_sd6[\"Cluster\"] = kmeans.fit_predict(X)"
      ],
      "metadata": {
        "id": "avO1wG2sz74A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Finds the most common words in each cluster to infer industry labels"
      ],
      "metadata": {
        "id": "CTiOi0VOxLSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Extract Common Words from Each Cluster to Infer Industry Labels\n",
        "def get_most_common_words(cluster_number):\n",
        "    titles_in_cluster = df_sd6[df_sd6[\"Cluster\"] == cluster_number][\"Cleaned_Job_Title\"].tolist()\n",
        "    all_words = [word for title in titles_in_cluster for word in title]\n",
        "    most_common_words = [word for word, _ in Counter(all_words).most_common(3)]\n",
        "    return most_common_words\n",
        "\n",
        "# Assign inferred industry names based on most common words\n",
        "industry_names = {}\n",
        "for cluster in range(optimal_clusters):\n",
        "    common_words = get_most_common_words(cluster)\n",
        "    industry_names[cluster] = \"-\".join(common_words)  # Use common words as a proxy for industry\n",
        "\n",
        "df_sd6[\"Industry\"] = df_sd6[\"Cluster\"].map(industry_names)"
      ],
      "metadata": {
        "id": "z-zuZvyCz-gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Reduces job title embeddings to 2D using t-SNE for visualization\n",
        "* Colours each job title by its assigned cluster"
      ],
      "metadata": {
        "id": "-VOKAgfWxOZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Visualize t-SNE Representation\n",
        "tsne = TSNE(n_components=2, perplexity=15, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=df_sd6[\"Cluster\"], cmap='tab10', alpha=0.7)\n",
        "plt.colorbar(scatter, label=\"Cluster\")\n",
        "plt.title(\"t-SNE Visualization of Job Titles (Coloured by Cluster)\")\n",
        "plt.xlabel(\"t-SNE Dimension 1\")\n",
        "plt.ylabel(\"t-SNE Dimension 2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gISfajsi0ALR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Creates a hierarchical clustering dendrogram for job titles\n",
        "* Colors clusters and adds a legend dynamically"
      ],
      "metadata": {
        "id": "Hh93XRCgxT8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# Generate Dendrogram\n",
        "linkage_matrix = linkage(X, method='ward')\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "dendro = dendrogram(\n",
        "    linkage_matrix,\n",
        "    labels=df_sd6[\"JobTitle\"].values,\n",
        "    leaf_rotation=45,\n",
        "    leaf_font_size=8,\n",
        "    truncate_mode='lastp',\n",
        "    p=20,\n",
        "    color_threshold=0.5 * max(linkage_matrix[:, 2])\n",
        ")\n",
        "\n",
        "# Extract the actual colors used\n",
        "cluster_colors = set(dendro['leaves_color_list'])\n",
        "legend_patches = [mpatches.Patch(color=color, label=f\"Cluster {i+1}\") for i, color in enumerate(cluster_colors)]\n",
        "\n",
        "# Add a legend\n",
        "plt.legend(handles=legend_patches, title=\"Clusters\", loc='upper right')\n",
        "\n",
        "plt.title(\"Hierarchical Clustering Dendrogram for Job Titles (With Correct Legend)\")\n",
        "plt.xlabel(\"Job Title Clusters\")\n",
        "plt.ylabel(\"Cluster Distance\")\n",
        "plt.show()\n",
        "\n",
        "print()\n",
        "df_sd6['Industry'].unique()"
      ],
      "metadata": {
        "id": "O3JqPI2O0CwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4.4: Heatmap for new categorical features relationships of Dataset"
      ],
      "metadata": {
        "id": "YgDPLO4RQkOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cramers_v(confusion_matrix):\n",
        "    \"\"\"Calculate Cramér's V statistic for association between two categorical variables\"\"\"\n",
        "    chi2 = stats.chi2_contingency(confusion_matrix)[0]  # Get Chi-square value\n",
        "    n = confusion_matrix.sum().sum()  # Total observations\n",
        "    k = min(confusion_matrix.shape)  # Minimum of row/column count\n",
        "    return np.sqrt(chi2 / (n * (k - 1)))\n",
        "\n",
        "# List of categorical variables\n",
        "new_categorical_vars = ['Gender', 'EduLevel', 'JobTitle', 'Industry', 'Generation', 'Seniority']\n",
        "\n",
        "# Compute Cramér's V matrix\n",
        "cramers_v_matrix = pd.DataFrame(np.zeros((len(new_categorical_vars), len(new_categorical_vars))),\n",
        "                                index=new_categorical_vars, columns=new_categorical_vars)\n",
        "\n",
        "for i, var1 in enumerate(new_categorical_vars):\n",
        "    for j, var2 in enumerate(new_categorical_vars):\n",
        "        if i == j:\n",
        "            cramers_v_matrix.iloc[i, j] = 1.0  # Diagonal should be 1 (same variable)\n",
        "        else:\n",
        "            contingency_table = pd.crosstab(df_sd6[var1], df_sd6[var2])\n",
        "            cramers_v_matrix.iloc[i, j] = cramers_v(contingency_table)\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cramers_v_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Cramér's V Heatmap for Categorical Variables\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Iwc4OFMlPBzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Determine if Additional Features brings improvement to the Baseline Model"
      ],
      "metadata": {
        "id": "_0MrqPYYaf9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Define a function to encode, train, and evaluate models\n",
        "def evaluate_feature_set(feature_set_name, df, categorical_ordinal, categorical_nominal, numerical_cols):\n",
        "    # Drop high-cardinality features\n",
        "    df_sd7 = df.drop(columns=['JobTitle', 'BirthYear', 'Cleaned_Job_Title', 'Job_Title_Vector', 'Cluster'])\n",
        "\n",
        "    # Separate features and target\n",
        "    X = df_sd7.drop(columns=['Salary'])\n",
        "    y = df_sd7['Salary']\n",
        "\n",
        "    # Encode ordinal categorical variables\n",
        "    for col in categorical_ordinal:\n",
        "        le = LabelEncoder()\n",
        "        X[col] = le.fit_transform(X[col])\n",
        "\n",
        "    # OneHotEncoding for nominal categorical variables\n",
        "    ohe = OneHotEncoder(sparse_output=False)\n",
        "    X_encoded = ohe.fit_transform(X[categorical_nominal])\n",
        "    encoded_feature_names = ohe.get_feature_names_out(categorical_nominal)\n",
        "\n",
        "    # Scale numerical variables\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X[numerical_cols])\n",
        "\n",
        "    # Combine all transformed features\n",
        "    X_processed = np.hstack((X_scaled, X[categorical_ordinal].values, X_encoded))\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    X_final = pd.DataFrame(X_processed, columns=numerical_cols + categorical_ordinal + list(encoded_feature_names))\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train and evaluate Random Forest\n",
        "    rfr_model = RandomForestRegressor(random_state=42)\n",
        "    rfr_model.fit(X_train, y_train)\n",
        "    y_pred_rfr = rfr_model.predict(X_test)\n",
        "\n",
        "    # Train and evaluate XGBoost\n",
        "    xgb_model = XGBRegressor(n_estimators=200, learning_rate=0.05, random_state=42)\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "    y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "    # Store results in a dictionary\n",
        "    results = {\n",
        "        \"Feature Set\": feature_set_name,\n",
        "        \"Model\": [\"Random Forest\", \"XGBoost\"],\n",
        "        \"MAE\": [mean_absolute_error(y_test, y_pred_rfr), mean_absolute_error(y_test, y_pred_xgb)],\n",
        "        \"MSE\": [mean_squared_error(y_test, y_pred_rfr), mean_squared_error(y_test, y_pred_xgb)],\n",
        "        \"R² Score\": [r2_score(y_test, y_pred_rfr), r2_score(y_test, y_pred_xgb)]\n",
        "    }\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Define feature combinations to test, including the baseline (no additional features)\n",
        "feature_combinations = [\n",
        "    (\"Baseline (No Additional Features)\", [\"EduLevel\"], [\"Gender\"], [\"Age\", \"YrsExp\"]),\n",
        "    (\"Generation\", [\"EduLevel\", \"Generation\"], [\"Gender\"], [\"Age\", \"YrsExp\"]),\n",
        "    (\"Seniority\", [\"EduLevel\", \"Seniority\"], [\"Gender\"], [\"Age\", \"YrsExp\"]),\n",
        "    (\"Generation + Seniority\", [\"EduLevel\", \"Generation\", \"Seniority\"], [\"Gender\"], [\"Age\", \"YrsExp\"]),\n",
        "    (\"Industry\", [\"EduLevel\"], [\"Gender\", \"Industry\"], [\"Age\", \"YrsExp\"]),\n",
        "    (\"Generation + Industry\", [\"EduLevel\", \"Generation\"], [\"Gender\", \"Industry\"], [\"Age\", \"YrsExp\"]),\n",
        "    (\"Seniority + Industry\", [\"EduLevel\", \"Seniority\"], [\"Gender\", \"Industry\"], [\"Age\", \"YrsExp\"]),\n",
        "    (\"All Features\", [\"EduLevel\", \"Generation\", \"Seniority\"], [\"Gender\", \"Industry\"], [\"Age\", \"YrsExp\"])\n",
        "]\n",
        "\n",
        "# Run evaluations\n",
        "results_df = pd.DataFrame()\n",
        "for feature_name, cat_ord, cat_nom, num_cols in feature_combinations:\n",
        "    results_df = pd.concat([results_df, evaluate_feature_set(feature_name, df_sd6, cat_ord, cat_nom, num_cols)])\n",
        "\n",
        "# Display results\n",
        "display(results_df)\n",
        "print()\n",
        "display(results_df[results_df['Model'] == 'Random Forest'])\n",
        "print()\n",
        "display(results_df[results_df['Model'] == 'XGBoost'])\n"
      ],
      "metadata": {
        "id": "0E4ut4URahas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfM1IBU1kBKn"
      },
      "source": [
        "#Step 6: Machine Learning Algorithm Implementation with Additional Features (Run Step 4 Before this)\n",
        "\n",
        "1. We will encode the relevant columns of data that fit our problem statement, this includes our new Additional Features\n",
        "2. Next, we will spilt the Dataset\n",
        "3. Lastly, we will run the best 2 Machine Learning Algorithm we have identified in Step 3 to see if the model improves\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6.1: Encode and Scale all Three Additional Features ('Generation'+'Seniority'+'Industry')"
      ],
      "metadata": {
        "id": "d8VwNdJvSTDH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3Q4G7lmn8C2"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
        "\n",
        "# Drop 'JobTitle' (high cardinality, too specific)\n",
        "df_sd8 = df_sd6.drop(columns=['JobTitle', 'BirthYear', 'Cleaned_Job_Title', 'Job_Title_Vector', 'Cluster'])\n",
        "\n",
        "# Separate features and target\n",
        "X = df_sd8.drop(columns=['Salary'])\n",
        "y = df_sd8['Salary']\n",
        "\n",
        "# Identify feature types\n",
        "categorical_ordinal = ['EduLevel', 'Generation', 'Seniority']  # Ordinal categories\n",
        "categorical_nominal = ['Gender', 'Industry']  # Nominal categories\n",
        "numerical_cols = ['Age', 'YrsExp']\n",
        "\n",
        "# Apply Label Encoding to ordinal categorical variables\n",
        "for col in categorical_ordinal:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Apply OneHotEncoding to nominal categorical variables\n",
        "ohe = OneHotEncoder(sparse_output=False)  # Avoid dummy variable trap\n",
        "X_encoded = ohe.fit_transform(X[categorical_nominal])\n",
        "encoded_feature_names = ohe.get_feature_names_out(categorical_nominal)\n",
        "\n",
        "# Scale numerical variables\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X[numerical_cols])\n",
        "\n",
        "# Combine all transformed features\n",
        "X_processed = np.hstack((X_scaled, X[categorical_ordinal].values, X_encoded))\n",
        "\n",
        "# Convert to DataFrame for clarity\n",
        "X_final = pd.DataFrame(X_processed, columns=numerical_cols + categorical_ordinal + list(encoded_feature_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6.2: Machine Learning Algorithms"
      ],
      "metadata": {
        "id": "fJxVXqEKPz8f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYsyamil2EGe"
      },
      "source": [
        "Spilting of Training Data (Must Run before Algorithm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jutDOYBp2Hiy"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Train-test split (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZUhNvkU2Q6m"
      },
      "source": [
        "Algorithm 3: RandomForestRegressor (Train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONoa5t0vw7jL"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "# rfr_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rfr_model = RandomForestRegressor(random_state=42)\n",
        "rfr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict salaries on the test set\n",
        "y_pred_rfr = rfr_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "mae_rfr = mean_absolute_error(y_test,y_pred_rfr)\n",
        "mse_rfr = mean_squared_error(y_test, y_pred_rfr)    # Mean Squared Error\n",
        "r2_rfr = r2_score(y_test, y_pred_rfr)\n",
        "\n",
        "# Print results\n",
        "print(\"Random Forest Regressor Performance:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_rfr:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_rfr:.2f}\")\n",
        "print(f\"R² Score: {r2_rfr:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na1fJ-QtY5sR"
      },
      "source": [
        "Algorithm 4: XGBoost (Train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egbcclyJY71s"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "xgb_model = XGBRegressor(n_estimators=200, learning_rate=0.05, random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict salaries on the test set\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost Performance\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_xgb:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_xgb:.2f}\")\n",
        "print(f\"R² Score: {r2_xgb:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT80b-AVIrC1"
      },
      "source": [
        "# Step 7: Fine-Tunning Machine Learning Algorithm (Run Step 6 Before this)\n",
        "\n",
        "*   Pick either Exhaustive or Faster\n",
        "*   According to your Algorithm of your Choice\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpWA9oOCIXpA"
      },
      "source": [
        "## Step 7.1: (Choose 1 of 2): Fine-Tuning with Grid Search (Exhaustive)\n",
        "*   Grid Search tests all possible combinations of specified hyperparameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithm 3**"
      ],
      "metadata": {
        "id": "M9WDCD1-IMDF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmEZm1eTH7Nc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "fine_tune_choice = 1\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid_rfr = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Perform Grid Search\n",
        "grid_search_rfr = GridSearchCV(rfr_model, param_grid_rfr, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=2)\n",
        "grid_search_rfr.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and performance\n",
        "print(\"Best Parameters:\", grid_search_rfr.best_params_)\n",
        "print(\"Best MAE:\", -grid_search_rfr.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithm 4**"
      ],
      "metadata": {
        "id": "cboUUkPvIQsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "fine_tune_choice = 1\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'min_child_weight': [1, 3, 5],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "# Grid Search\n",
        "grid_search_xgb = GridSearchCV(xgb_model, param_grid_xgb, scoring='neg_mean_absolute_error', cv=5, verbose=2, n_jobs=-1)\n",
        "grid_search_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Best Parameters\n",
        "print(\"Best Parameters:\", grid_search_xgb.best_params_)\n",
        "print(\"Best MAE:\", -grid_search_xgb.best_score_)"
      ],
      "metadata": {
        "id": "RZBuPPD3ITko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkKbl-XNJNOU"
      },
      "source": [
        "## Step 7.1: (Choose 2 of 2): Fine-Tuning with Random Search (Faster)\n",
        "*   Random Search randomly samples hyperparameters instead of testing all combinations."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Algorithm 3**"
      ],
      "metadata": {
        "id": "uxJpGrjaIifY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvoRUi9vJT8g"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "fine_tune_choice = 2\n",
        "\n",
        "# Define the parameter grid with distributions\n",
        "param_dist_rfr = {\n",
        "    'n_estimators': randint(100, 500),\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': randint(2, 20),\n",
        "    'min_samples_leaf': randint(1, 10),\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Perform Randomized Search\n",
        "random_search_rfr = RandomizedSearchCV(rfr_model, param_dist_rfr, n_iter=20, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=2, random_state=42)\n",
        "random_search_rfr.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and performance\n",
        "print(\"Best Parameters:\", random_search_rfr.best_params_)\n",
        "print(\"Best MAE:\", -random_search_rfr.best_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh_lJPVXoCmO"
      },
      "source": [
        " **Algorithm 4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlZK-r1uoCyq"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "fine_tune_choice = 2\n",
        "\n",
        "# Define the parameter distribution\n",
        "param_dist_xgb = {\n",
        "    'n_estimators': randint(100, 500),\n",
        "    'learning_rate': uniform(0.01, 0.3),\n",
        "    'max_depth': randint(3, 10),\n",
        "    'min_child_weight': randint(1, 10),\n",
        "    'subsample': uniform(0.7, 0.3),\n",
        "    'colsample_bytree': uniform(0.7, 0.3),\n",
        "    'gamma': uniform(0, 0.5),\n",
        "    'reg_alpha': uniform(0, 0.5),\n",
        "    'reg_lambda': uniform(0, 0.5)\n",
        "}\n",
        "\n",
        "# Randomized Search\n",
        "random_search_xgb = RandomizedSearchCV(xgb_model, param_dist_xgb, n_iter=20, scoring='neg_mean_absolute_error', cv=5, verbose=2, n_jobs=-1, random_state=42)\n",
        "random_search_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Best Parameters\n",
        "print(\"Best Parameters:\", random_search_xgb.best_params_)\n",
        "print(\"Best MAE:\", -random_search_xgb.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSE5Dow3LHVU"
      },
      "source": [
        "# Step 8: Retraining of Algorithm Model (Run only after Step 6)\n",
        "\n",
        "*   Find the Algorithm of Choice\n",
        "*   Retrain First\n",
        "*   Feature Importance Comparison\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8.1: **Algorithm 3**: Random Forest Regressor"
      ],
      "metadata": {
        "id": "H77NAegxI5qC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn9_N3eVLRyA"
      },
      "source": [
        "Retrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_8j4e_BLNnO"
      },
      "outputs": [],
      "source": [
        "if fine_tune_choice == 1:\n",
        "  best_params = grid_search_rfr.best_params_\n",
        "elif fine_tune_choice == 2:\n",
        "  best_params = random_search_rfr.best_params_\n",
        "\n",
        "best_model_rfr = RandomForestRegressor(**best_params, random_state=42)\n",
        "best_model_rfr.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_rfr = best_model_rfr.predict(X_test)\n",
        "\n",
        "mae_tuned = mean_absolute_error(y_test, y_pred_rfr)\n",
        "mse_tuned = mean_squared_error(y_test, y_pred_rfr)\n",
        "r2_tuned = r2_score(y_test, y_pred_rfr)\n",
        "\n",
        "print(f\"Tuned Model MAE: {mae_tuned:.2f}\")\n",
        "print(f\"Tuned Model MSE: {mse_tuned:.2f}\")\n",
        "print(f\"Tuned Model R² Score: {r2_tuned:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_94MLigBQN_B"
      },
      "source": [
        "View Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ihU3c6Po5U3"
      },
      "outputs": [],
      "source": [
        "# Feature importances from both models\n",
        "feature_importances_1 = rfr_model.feature_importances_\n",
        "feature_importances_2 = best_model_rfr.feature_importances_\n",
        "feature_names = X_final.columns\n",
        "\n",
        "# Sort feature importance values\n",
        "sorted_idx_1 = np.argsort(feature_importances_1)[::-1]\n",
        "sorted_idx_2 = np.argsort(feature_importances_2)[::-1]\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))  # 1 row, 2 columns\n",
        "\n",
        "# Plot first model's feature importance\n",
        "axes[0].bar(range(len(feature_importances_1)), feature_importances_1[sorted_idx_1], tick_label=np.array(feature_names)[sorted_idx_1])\n",
        "axes[0].set_xticklabels(np.array(feature_names)[sorted_idx_1], rotation=90)\n",
        "axes[0].set_xlabel(\"Feature\")\n",
        "axes[0].set_ylabel(\"Importance\")\n",
        "axes[0].set_title(\"Feature Importance in Random Forest (Before Tune)\")\n",
        "\n",
        "# Plot second model's feature importance\n",
        "axes[1].bar(range(len(feature_importances_2)), feature_importances_2[sorted_idx_2], tick_label=np.array(feature_names)[sorted_idx_2])\n",
        "axes[1].set_xticklabels(np.array(feature_names)[sorted_idx_2], rotation=90)\n",
        "axes[1].set_xlabel(\"Feature\")\n",
        "axes[1].set_ylabel(\"Importance\")\n",
        "axes[1].set_title(\"Feature Importance in Random Forest (After Tune)\")\n",
        "\n",
        "# Show the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 8.1: **Algorithm 4**: XGBoost"
      ],
      "metadata": {
        "id": "SePa6EvfI93X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4GxkUm2rqn5"
      },
      "source": [
        "Retrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wclSKDktrpLY"
      },
      "outputs": [],
      "source": [
        "if fine_tune_choice == 1:\n",
        "  best_params = grid_search_xgb.best_params_\n",
        "elif fine_tune_choice == 2:\n",
        "  best_params = random_search_xgb.best_params_\n",
        "\n",
        "best_model_xgb = XGBRegressor(**best_params, random_state=42)\n",
        "best_model_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_xgb = best_model_xgb.predict(X_test)\n",
        "\n",
        "mae_tuned = mean_absolute_error(y_test, y_pred_xgb)\n",
        "mse_tuned = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_tuned = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"Tuned Model MAE: {mae_tuned:.2f}\")\n",
        "print(f\"Tuned Model MSE: {mse_tuned:.2f}\")\n",
        "print(f\"Tuned Model R² Score: {r2_tuned:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Yoy3i4psFAP"
      },
      "source": [
        "View Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAq_lgFpsH8k"
      },
      "outputs": [],
      "source": [
        "# Feature importances from both models\n",
        "feature_importances_1 = xgb_model.feature_importances_\n",
        "feature_importances_2 = best_model_xgb.feature_importances_\n",
        "feature_names = X_final.columns\n",
        "\n",
        "# Sort feature importance values\n",
        "sorted_idx_1 = np.argsort(feature_importances_1)[::-1]\n",
        "sorted_idx_2 = np.argsort(feature_importances_2)[::-1]\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))  # 1 row, 2 columns\n",
        "\n",
        "# Plot first model's feature importance\n",
        "axes[0].bar(range(len(feature_importances_1)), feature_importances_1[sorted_idx_1], tick_label=np.array(feature_names)[sorted_idx_1])\n",
        "axes[0].set_xticklabels(np.array(feature_names)[sorted_idx_1], rotation=90)\n",
        "axes[0].set_xlabel(\"Feature\")\n",
        "axes[0].set_ylabel(\"Importance\")\n",
        "axes[0].set_title(\"Feature Importance in XGBRegressor (Before Tune)\")\n",
        "\n",
        "# Plot second model's feature importance\n",
        "axes[1].bar(range(len(feature_importances_2)), feature_importances_2[sorted_idx_2], tick_label=np.array(feature_names)[sorted_idx_2])\n",
        "axes[1].set_xticklabels(np.array(feature_names)[sorted_idx_2], rotation=90)\n",
        "axes[1].set_xlabel(\"Feature\")\n",
        "axes[1].set_ylabel(\"Importance\")\n",
        "axes[1].set_title(\"Feature Importance in XGBRegressor (After Tune)\")\n",
        "\n",
        "# Show the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgwN-4S1xQIQ"
      },
      "source": [
        "# Step 9: Exploring Combining of Two Algorithms (Run only after Step 7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QGs3z_wHVTD"
      },
      "outputs": [],
      "source": [
        "# Define Weights Based on Performance (higher R² → higher weight)\n",
        "w_rfr = r2_score(y_test, y_pred_rfr)\n",
        "w_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "# Normalize Weights\n",
        "total_weight = w_rfr + w_xgb\n",
        "w_rfr /= total_weight\n",
        "w_xgb /= total_weight\n",
        "\n",
        "# Weighted Average\n",
        "y_pred_weighted = (w_rfr * y_pred_rfr) + (w_xgb * y_pred_xgb)\n",
        "\n",
        "# Evaluate Performance\n",
        "mae = mean_absolute_error(y_test, y_pred_weighted)\n",
        "mse = mean_squared_error(y_test, y_pred_weighted)\n",
        "r2 = r2_score(y_test, y_pred_weighted)\n",
        "\n",
        "print(\"Ensemble Model Performance (Weighted Averaging):\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y70e36RPGtX4"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "\n",
        "# Extract the best parameters from tuning\n",
        "rf_best_params = {k: v for k, v in best_params.items() if k in ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'max_features']}\n",
        "xgb_best_params = {k: v for k, v in best_params.items() if k in ['n_estimators', 'learning_rate', 'max_depth', 'min_child_weight', 'subsample', 'colsample_bytree']}\n",
        "\n",
        "# Define Base Models (Using Correct Parameters for Each Model)\n",
        "base_models = [\n",
        "    ('rf', RandomForestRegressor(**rf_best_params, random_state=42)),\n",
        "    ('xgb', XGBRegressor(**xgb_best_params, random_state=42))\n",
        "]\n",
        "\n",
        "# Meta Model (Final Predictor)\n",
        "meta_model = LinearRegression()\n",
        "\n",
        "# Stacking Regressor\n",
        "stacking_reg = StackingRegressor(estimators=base_models, final_estimator=meta_model, cv=5)\n",
        "stacking_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on Test Data\n",
        "y_pred_stack = stacking_reg.predict(X_test)\n",
        "\n",
        "# Evaluate Performance\n",
        "mae = mean_absolute_error(y_test, y_pred_stack)\n",
        "mse = mean_squared_error(y_test, y_pred_stack)\n",
        "r2 = r2_score(y_test, y_pred_stack)\n",
        "\n",
        "print(\"Stacking Model Performance:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGzTV7QjU9Ay"
      },
      "source": [
        "# Step 10: Save and Deploy the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYeDaNf9X1kz"
      },
      "source": [
        "Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur_W0fzMWzGS"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(stacking_reg, 'stacking_regressor.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9y70P1jX3Ce"
      },
      "source": [
        "Load the Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bO12mWJEX4-_"
      },
      "outputs": [],
      "source": [
        "# Load the trained model\n",
        "stacking_reg = joblib.load('stacking_regressor.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dLHRuRWYCKC"
      },
      "source": [
        " Prepare New Data for Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTN8evLIYIse"
      },
      "outputs": [],
      "source": [
        "# Example new data (should match feature columns of X_train)\n",
        "new_data = {\n",
        "    'Age': [30],  # Example Age\n",
        "    'YrsExp': [5],  # Example Years of Experience\n",
        "    'EduLevel': ['BachelorDegree'],  # Example Education Level\n",
        "    'Gender': ['Male'],  # Example Gender\n",
        "    'Industry': ['Technology & IT Services'],  # Example Industry\n",
        "    'Generation': ['Millennials'],  # Example Generation\n",
        "    'Seniority': ['Junior']  # Example Seniority Level\n",
        "}\n",
        "\n",
        "# Convert new data to DataFrame\n",
        "df_new = pd.DataFrame(new_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLfVaMLUYKjw"
      },
      "source": [
        "Preprocess New Data (Apply Same Encoding and Scaling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YSQktoKYN46"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
        "\n",
        "# Load encoders used during training\n",
        "le_education = LabelEncoder()\n",
        "le_education.classes_ = np.array(['HighSchool', 'BachelorDegree', 'MasterDegree', 'PhD'])\n",
        "df_new['EduLevel'] = le_education.transform(df_new['EduLevel'])\n",
        "\n",
        "le_generation = LabelEncoder()\n",
        "le_generation.classes_ = np.array(['SilentGeneration', 'BabyBoomer', 'GenX', 'Millennials', 'GenZ'])\n",
        "df_new['Generation'] = le_generation.transform(df_new['Generation'])\n",
        "\n",
        "le_seniority = LabelEncoder()\n",
        "le_seniority.classes_ = np.array(['Entry', 'Junior', 'Mid', 'Senior', 'Executive'])\n",
        "df_new['Seniority'] = le_seniority.transform(df_new['Seniority'])\n",
        "\n",
        "# OneHotEncode Gender\n",
        "ohe_gender = OneHotEncoder(categories=[['Female', 'Male']], drop='first', sparse_output=False)\n",
        "gender_encoded = ohe_gender.fit_transform(df_new[['Gender']])  # Apply same encoding as in training\n",
        "gender_columns = ohe_gender.get_feature_names_out(['Gender'])\n",
        "df_gender_encoded = pd.DataFrame(gender_encoded, columns=gender_columns)\n",
        "\n",
        "# OneHotEncode Industry\n",
        "ohe_industry = OneHotEncoder(categories=[[\n",
        "    'Marketing & Communications', 'Research & Customer Support',\n",
        "    'Executive & Senior Management', 'Technology & IT Services',\n",
        "    'Data Science & Analytics', 'Healthcare & Life Sciences',\n",
        "    'Finance & Accounting', 'Operations & Supply Chain',\n",
        "    'Education & Training', 'Sales & Business Development',\n",
        "    'Human Resources & Administration', 'Business Strategy & Consulting'\n",
        "]], drop='first', sparse_output=False)\n",
        "\n",
        "industry_encoded = ohe_industry.fit_transform(df_new[['Industry']])  # Ensure same encoding\n",
        "industry_columns = ohe_industry.get_feature_names_out(['Industry'])\n",
        "df_industry_encoded = pd.DataFrame(industry_encoded, columns=industry_columns)\n",
        "\n",
        "# **Ensure all expected columns are present (to match training features)**\n",
        "df_gender_encoded = df_gender_encoded.reindex(columns=['Gender_Male'], fill_value=0)  # Only 'Male' exists due to drop='first'\n",
        "df_industry_encoded = df_industry_encoded.reindex(columns=industry_columns, fill_value=0)\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "df_new[['Age', 'YrsExp']] = scaler.fit_transform(df_new[['Age', 'YrsExp']])\n",
        "\n",
        "# **Combine processed data**\n",
        "df_processed = pd.concat([\n",
        "    df_new[['Age', 'YrsExp', 'EduLevel', 'Generation', 'Seniority']],\n",
        "    df_gender_encoded, df_industry_encoded\n",
        "], axis=1)\n",
        "\n",
        "# **Ensure final feature order matches training data**\n",
        "df_processed = df_processed.reindex(columns=X_final.columns, fill_value=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJw7qtYetSfO"
      },
      "source": [
        "Test if any missing features between user and saved model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gai08Mw0mBR9"
      },
      "outputs": [],
      "source": [
        "# Get expected feature names from training data\n",
        "expected_features = X_final.columns.tolist()\n",
        "\n",
        "# Get actual feature names from the new input\n",
        "actual_features = df_processed.columns.tolist()\n",
        "\n",
        "# Find the missing feature\n",
        "missing_features = list(set(expected_features) - set(actual_features))\n",
        "extra_features = list(set(actual_features) - set(expected_features))\n",
        "\n",
        "print(f\"Missing feature(s): {missing_features}\")\n",
        "print(f\"Extra feature(s): {extra_features}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd_woAqOYRbb"
      },
      "source": [
        "Make Predictions\n",
        "\n",
        "*  Once preprocessing is complete, pass the transformed data to your trained model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UcAga_CYQqH"
      },
      "outputs": [],
      "source": [
        "predicted_salary = stacking_reg.predict(df_processed.to_numpy())\n",
        "\n",
        "# Calculate absolute error between predicted salary and actual test values\n",
        "errors = np.abs(y_test - predicted_salary[0])\n",
        "\n",
        "# Find the closest actual salary\n",
        "closest_index = np.argmin(errors)\n",
        "closest_actual_salary = y_test.iloc[closest_index]\n",
        "\n",
        "# Calculate percentage error\n",
        "percentage_error = (abs(predicted_salary[0] - closest_actual_salary) / closest_actual_salary) * 100\n",
        "\n",
        "# Print results\n",
        "print(f\"Predicted Salary: {predicted_salary[0]:,.2f}\")\n",
        "print(f\"Closest Actual Salary: {closest_actual_salary:,.2f}\")\n",
        "print(f\"Percentage Error: {percentage_error:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrjHjy11sr2I"
      },
      "source": [
        "ScatterPlot to Demonstrate Accuracy\n",
        "\n",
        "*   Blue dots → Predictions from the test set\n",
        "*   Red dashed line → Ideal prediction line (y = x)\n",
        "*   Red 'X' marker → The newly predicted salary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCBJwNlfrZtE"
      },
      "outputs": [],
      "source": [
        "# Predict salaries for test set\n",
        "y_pred_test = stacking_reg.predict(X_test)\n",
        "\n",
        "# Create the scatter plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, y_pred_test, alpha=0.4, label=\"Model Predictions\")\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label=\"Perfect Prediction (y = x)\")\n",
        "\n",
        "# Mark the new predicted salary\n",
        "plt.scatter(predicted_salary[0], predicted_salary[0], color='red', s=150, marker='x', label=\"New Prediction\")\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel(\"Actual Salary\")\n",
        "plt.ylabel(\"Predicted Salary\")\n",
        "plt.title(\"Predicted vs. Actual Salary\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll-AV-pisoET"
      },
      "source": [
        "2D histogram (hexbin plot) to show density:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AX_7Cs4wseGn"
      },
      "outputs": [],
      "source": [
        "plt.hexbin(y_test, y_pred_test, gridsize=50, cmap='Blues', alpha=0.8)\n",
        "plt.colorbar(label='Density of Points')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MAQb5CduErS"
      },
      "outputs": [],
      "source": [
        "# Plot error distribution\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.hist(errors, bins=30, color='skyblue', alpha=0.7)\n",
        "plt.axvline(x=np.abs(predicted_salary[0] - closest_actual_salary), color='red', linestyle='dashed', linewidth=2, label=\"New Prediction Error\")\n",
        "plt.xlabel(\"Absolute Error\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Prediction Errors\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z05nvCYnwdY2"
      },
      "source": [
        "#With Love, On Hold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO3J-_DJqw6J"
      },
      "source": [
        "Encoding of Gender, Education, Generation, Seniority"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYdJ9DY_uiFf"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "df_sd4['Gender'] = df_sd4['Gender']\n",
        "\n",
        "# Encoding 'Gender'\n",
        "le_gender = LabelEncoder()  # Create a LabelEncoder for Gender\n",
        "df_sd4['Gender_Encoded'] = le_gender.fit_transform(df_sd4['Gender']) # Encode and add a new column\n",
        "\n",
        "# Define the desired education levels\n",
        "desired_levels = [\"Bachelor's\", \"High School\", \"Master's\", 'PhD']\n",
        "\n",
        "# Create a new column 'Education_Cleaned'\n",
        "df_sd4['Education_Cleaned'] = df_sd4['EduLevel'].apply(lambda x: x if x in desired_levels else 'Other')\n",
        "\n",
        "# Now apply Label Encoding to the cleaned column\n",
        "le_education = LabelEncoder()\n",
        "df_sd4['Education_Encoded'] = le_education.fit_transform(df_sd4['Education_Cleaned'])\n",
        "\n",
        "# Display the mappings (optional)\n",
        "print(\"Gender Mapping:\", dict(zip(le_gender.classes_, le_gender.transform(le_gender.classes_))))\n",
        "print(\"Number of entries for Gender_Encoded:\", df_sd4['Gender_Encoded'].count())\n",
        "print(\"Education Level Mapping:\", dict(zip(le_education.classes_, le_education.transform(le_education.classes_))))\n",
        "print(\"Number of entries for Education_Encoded:\", df_sd4['Education_Encoded'].count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsFyMkE_oDpM"
      },
      "source": [
        "Correlation Heat Map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rurlt6E6wExR"
      },
      "outputs": [],
      "source": [
        "# Show correlation heat map and matrix\n",
        "df_corr = df_sd4[['Age','YrsExp','Salary','Gender_Encoded','Education_Encoded']]\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "corrmat = df_corr.corr()\n",
        "\n",
        "# Title\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "\n",
        "# Generate the heatmap\n",
        "sns.heatmap(corrmat, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Identify duplicated rows\n",
        "df_duplicates_all = df_sd2[df_sd2.duplicated(keep=False)]  # 'keep=False' keeps all duplicates (including the first occurrence)\n",
        "\n",
        "df_duplicates_all.to_csv('duplicate_data_all.csv', index=False)\n",
        "'''\n"
      ],
      "metadata": {
        "id": "XXNx1Uvf-IhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1 of 2: Clustering JobTitles to Industry**\n",
        "\n",
        "Extract Unique JobTitles & Preprocess Text\n",
        "\n",
        "*   Lowercase transformation\n",
        "*   Special character removal (if needed)\n",
        "*   Convert text into numerical format using TF-IDF vectorisation\n",
        "\n",
        "Determine Optimal Number of Clusters\n",
        "\n",
        "*   Use Elbow Method to find the best cluster number based on inertia.\n",
        "*   Use Silhouette Score Analysis to validate the best choice."
      ],
      "metadata": {
        "id": "Ot0t0IrTm196"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "# Extract unique job titles again after reset\n",
        "job_titles = df_sd6['JobTitle'].dropna().unique()\n",
        "\n",
        "# Convert job titles to lowercase for standardization\n",
        "cleaned_titles = [title.lower() for title in job_titles]\n",
        "\n",
        "# Convert text data into numerical format using TF-IDF vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_job = vectorizer.fit_transform(cleaned_titles)\n",
        "\n",
        "# Define cluster range dynamically\n",
        "min_k = 2  # Minimum clusters\n",
        "max_k = 15  # Maximum clusters\n",
        "\n",
        "# Initialize lists to store results\n",
        "inertia = []\n",
        "silhouette_scores = []\n",
        "gap_values = []\n",
        "\n",
        "# Compute Gap Statistic method\n",
        "def gap_statistic(X_job, k):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X_job)\n",
        "    cluster_dispersion = np.mean([np.min(pairwise_distances(X_job, [center]), axis=1) for center in kmeans.cluster_centers_])\n",
        "\n",
        "    # Generate reference random dataset\n",
        "    random_X = np.random.uniform(X_job.min(), X_job.max(), X_job.shape)\n",
        "    random_dispersion = np.mean([np.min(pairwise_distances(random_X, [center]), axis=1) for center in kmeans.cluster_centers_])\n",
        "\n",
        "    return random_dispersion - cluster_dispersion\n",
        "\n",
        "# Loop through cluster range and compute values\n",
        "cluster_range = range(min_k, max_k)\n",
        "for k in cluster_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X_job)\n",
        "\n",
        "    # Store values\n",
        "    inertia.append(kmeans.inertia_)\n",
        "    silhouette_scores.append(silhouette_score(X_job, kmeans.labels_))\n",
        "    gap_values.append(gap_statistic(X_job, k))\n",
        "\n",
        "# Determine optimal clusters based on each method\n",
        "optimal_k_elbow = cluster_range[np.argmax(np.diff(inertia))]  # Elbow method approximation\n",
        "optimal_k_silhouette = cluster_range[np.argmax(silhouette_scores)]  # Best silhouette score\n",
        "optimal_k_gap = cluster_range[np.argmax(gap_values)]  # Best gap statistic\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(18, 5))\n",
        "\n",
        "# Elbow Method Plot\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(cluster_range, inertia, marker='o', linestyle='--', color='blue')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Inertia (Sum of Squared Distances)')\n",
        "plt.title('Elbow Method')\n",
        "\n",
        "# Silhouette Score Plot\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(cluster_range, silhouette_scores, marker='o', linestyle='--', color='red')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score Analysis')\n",
        "\n",
        "# Gap Statistic Plot\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(cluster_range, gap_values, marker='o', linestyle='--', color='green')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Gap Statistic')\n",
        "plt.title('Gap Statistic Analysis')\n",
        "\n",
        "# Display plots\n",
        "plt.show()\n",
        "\n",
        "# Print the optimal values found by each method\n",
        "print(\"Optimal values found by each method:\")\n",
        "optimal_k_elbow, optimal_k_silhouette, optimal_k_gap"
      ],
      "metadata": {
        "id": "vhYEvUkImxU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further Testing to determine which k to use"
      ],
      "metadata": {
        "id": "ljnl9LTrmrE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply K-Means clustering with k=12 (Refined Job Categories)\n",
        "kmeans_refined = KMeans(n_clusters=12, random_state=42, n_init=10)\n",
        "clusters_refined = kmeans_refined.fit_predict(X_job)\n",
        "\n",
        "# Apply K-Means clustering with k=2 (Broader Job Categories)\n",
        "kmeans_broad = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
        "clusters_broad = kmeans_broad.fit_predict(X_job)\n",
        "\n",
        "# Map job titles to clusters for both cases\n",
        "df_sd6['Industry (Refined - k=12)'] = df_sd6['JobTitle'].map(dict(zip(job_titles, clusters_refined)))\n",
        "df_sd6['Industry (Broad - k=2)'] = df_sd6['JobTitle'].map(dict(zip(job_titles, clusters_broad)))\n",
        "\n",
        "# Define industry mappings (Placeholder Names)\n",
        "industry_mapping_refined = {i: f\"Industry {i+1}\" for i in range(12)}\n",
        "industry_mapping_broad = {0: \"General Industry A\", 1: \"General Industry B\"}\n",
        "\n",
        "# Apply mappings\n",
        "df_sd6['Industry (Refined - k=12)'] = df_sd6['Industry (Refined - k=12)'].map(industry_mapping_refined)\n",
        "df_sd6['Industry (Broad - k=2)'] = df_sd6['Industry (Broad - k=2)'].map(industry_mapping_broad)\n",
        "\n",
        "# Display the DataFrame with both refined (k=12) and broad (k=2) industries\n",
        "df_sd6[['JobTitle', 'Industry (Refined - k=12)', 'Industry (Broad - k=2)']]\n",
        "\n",
        "# df_sd6.to_csv('cluster_compare_job_titles.csv', index=False)"
      ],
      "metadata": {
        "id": "HBVTtR__mlwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group job titles by their assigned cluster (Industry k=12)\n",
        "industry_job_distribution = df_sd6.groupby(\"Industry (Refined - k=12)\")[\"JobTitle\"].unique()\n",
        "\n",
        "# Convert to DataFrame for better readability\n",
        "df_industry_job = industry_job_distribution.reset_index()\n",
        "df_industry_job.columns = [\"Cluster\", \"Job Titles\"]\n",
        "\n",
        "df_industry_job"
      ],
      "metadata": {
        "id": "T6WOXcJDmiCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2 of 2: Clustering JobTitles to Industry**\n",
        "\n",
        "Apply K-Means Clustering with Optimal Clusters\n",
        "\n",
        "*   Fit the model using the finalised number of clusters.\n",
        "*   Assign each job title to a cluster.\n",
        "\n",
        "Map Clusters to Industry Names\n",
        "\n",
        "*   Define a manual mapping from cluster numbers to industry labels.\n",
        "*   Apply this mapping to assign industry names to job titles.\n",
        "\n",
        "Update the Original DataFrame\n",
        "\n",
        "*   Merge the clustered job titles back into the dataset with a new column \"Industry\""
      ],
      "metadata": {
        "id": "wkn-hlnlmenc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove column for broad\n",
        "df_sd6 = df_sd6.drop('Industry (Broad - k=2)', axis=1)\n",
        "df_sd6 = df_sd6.drop('Industry (Refined - k=12)', axis=1)\n",
        "\n",
        "# Select optimal clusters based on highest silhouette score\n",
        "optimal_clusters = cluster_range[np.argmax(silhouette_scores)]\n",
        "\n",
        "# Apply K-Means clustering with the optimized number of clusters\n",
        "kmeans_optimal = KMeans(n_clusters=optimal_clusters, random_state=42, n_init=10)\n",
        "clusters_optimal = kmeans_optimal.fit_predict(X_job)\n",
        "\n",
        "# Map job titles to clusters\n",
        "job_cluster_mapping = dict(zip(job_titles, clusters_optimal))\n",
        "df_sd6['Industry'] = df_sd6['JobTitle'].map(job_cluster_mapping)\n",
        "\n",
        "# Define refined industry names based on job title distribution\n",
        "industry_mapping_refined = {\n",
        "    0: \"Marketing & Communications\",\n",
        "    1: \"Research & Customer Support\",\n",
        "    2: \"Executive & Senior Management\",\n",
        "    3: \"Technology & IT Services\",\n",
        "    4: \"Data Science & Analytics\",\n",
        "    5: \"Healthcare & Life Sciences\",\n",
        "    6: \"Finance & Accounting\",\n",
        "    7: \"Operations & Supply Chain\",\n",
        "    8: \"Education & Training\",\n",
        "    9: \"Sales & Business Development\",\n",
        "    10: \"Human Resources & Administration\",\n",
        "    11: \"Business Strategy & Consulting\"\n",
        "}\n",
        "# Apply industry mapping\n",
        "df_sd6['Industry'] = df_sd6['Industry'].map(industry_mapping_refined)\n",
        "\n",
        "print(df_sd6)"
      ],
      "metadata": {
        "id": "GcIaWEhsmb0G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}